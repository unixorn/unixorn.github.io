[{"content":"In the next few posts, I\u0026rsquo;m going to document how to set up Home Assistant (HA) from scratch. We\u0026rsquo;re going to want to protect the admin UI interfaces for HA and its support services with SSL, and add authentication to services that don\u0026rsquo;t provide it themselves.\nWe\u0026rsquo;re going to do this with Nginx Proxy Manager because it has built in support for using LetsEncrypt to obtain free SSL certificates, supports adding authentication to services that don\u0026rsquo;t do it themselves, and is overall easy to use.\nBefore I start writing more Home Assitant articles, let\u0026rsquo;s set up a SSL proxy server to keep everything secure.\nWe\u0026rsquo;re going to run the proxy server and the services behind it in containers with docker-compose to make things easier.\nPre-requisites A linux machine with docker installed that doesn\u0026rsquo;t already have something running on port 80 A domain that is using Route 53 for DNS A server on your network with a static IP, for example 10.1.2.3 A DNS entry pointing at your server, like demo.yourdomain.com Setup All screen shots and other instructions are valid as of 2023-07-22 when I wrote this post.\nRather than set up a web server and expose it to the internet so that LetsEncrypt can validate that you own the domain, we\u0026rsquo;re going to configure nginx-proxy-manager to use a DNS01 challenge. This lets LetsEncrypt validate ownership of your domain by special records that will be added to your domain\u0026rsquo;s DNS entries during certificate creation / renewal by our nginx-proxy-manager container.\nnginx-proxy-manager supports many DNS providers. I use Route 53 so I\u0026rsquo;ll use that for this article.\nSet up a domain in AWS Route 53 If you already have a domain, you can transfer it to Route 53. I think I pay about $1.50 a month for the domains I host there. And that\u0026rsquo;s in total, not per domain. My homelab doesn\u0026rsquo;t get a ton of DNS queries each month.\nIf you don\u0026rsquo;t own a domain or don\u0026rsquo;t want to transfer one you own to R53, you can use Amazon for your registrar. They support many tlds, and some of the ones they support cost less than $13 a year - in my opinion it\u0026rsquo;s worth a dollar a month to have a separate domain for a homelab.\nSet up Route 53 You don\u0026rsquo;t want to use your root IAM credentials with nginx-proxy-manager. It is never a good idea to use root IAM credentials for anything. Instead, we\u0026rsquo;re going to create an IAM user that only has privileges to affect your Route53-hosted domains.\nThis is pretty tedious, so it\u0026rsquo;s a good thing you only need to do it once. Log in to the AWS console and select Identity and Access Management (IAM).\nCreate an IAM policy We\u0026rsquo;re going to start by creating a policy that grants control over Route53, so select Policies from the sidebar, then hit the Create Policy button.\nCreating the policy first makes it easier to attach to the IAM group when we create it in the next step.\nI\u0026rsquo;ve already created a working policy, so instead of manually adding permissions, click JSON to the right of where it says Policy Editor and paste in this JSON snippet\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;route53:ListHostedZonesByName\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:route53:::hostedzone/ZG2ZIACD7OSSR\u0026#34; ] } ] } If you want to restrict it to only controlling a specific domain, update the \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; line - that\u0026rsquo;s out of scope for this post, though.\nAs of 2023-07-22, it should look like this:\nClick Next, and give your new policy a name and a description, for example blog-letsencrypt.\nCreate an IAM group Now you need to create a group, so click User Groups in the side bar, then hit the Create Group button. Give it a name like blog-le-users\nUnder Attach permissions policy* select the blog-letsencrypt policy you just created.\nHit the Create Group button on the bottom right of the page.\nCreate an IAM user Now that the group and policy are created, you can create your r53 IAM user. Click Users in the side bar, then click the Add Users button on the right side of the screen.\nGive it a name like r53-acme-user and click Next.\nDon\u0026rsquo;t bother to check the Provide user access to the AWS Management Console checkbox, this user will only be used by your proxy manager. Select the blog-le-users group\nClick Next again.\nYou\u0026rsquo;ll see a review and create page:\nClick Create user on the bottom right of the page.\nOne last thing - you\u0026rsquo;ll need to create access credentials for the user. Click on Users in the side bar again, then your brand new r53-acme-user user.\nYou\u0026rsquo;ll see an info page about the user, click the Security Credentials tab\nScroll down to Access Keys, and click Create access key.\nClick Other on the next page\nClick Next. Put in letsencrypt for the description,and click Create access key again.\nYou\u0026rsquo;ll see something like\nYou only get one chance to see the secret key, so store it in your password manager. It isn\u0026rsquo;t a huge deal if you lose it, you can always create another access key for your user. Copy the access key \u0026amp; secret keys into your password manager for later use.\nInstall nginx-proxy-manager Now that you have the DNS domain on Route 53 and have created an IAM user with rights to update it, you can set up nginx-proxy-manager and have it generate LetsEncrypt SSL certificates.\nFor the SSL proxy, I like to set it to use an external docker network. This lets you run other docker containers behind it easily, without having to cram them all into the same docker-compose.yaml file.\nCreating the network is easy - docker network create ssl_proxy_network and we\u0026rsquo;re good to go.\nHere\u0026rsquo;s the docker-compose.yaml file I use to start nginx-proxy-manager. As of 2023-07-21, there\u0026rsquo;s a bug getting Route 53 to work with the latest tag, but it does work with the docker image with github-pr-2971 tag. I\u0026rsquo;ll update this post when that fix gets merged upstream.\nConfigure docker-compose version: \u0026#39;3\u0026#39; services: nginx-proxy-manager: # image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; # Use github-pr-2971 until the fix is merged image: jc21/nginx-proxy-manager:github-pr-2971 restart: unless-stopped container_name: nginx-proxy-manager ports: - \u0026#39;80:80\u0026#39; - \u0026#39;81:81\u0026#39; - \u0026#39;443:443\u0026#39; volumes: - ./nginx/data:/data - ./nginx/letsencrypt:/etc/letsencrypt - /etc/hostname:/etc/hostname:ro - /etc/localtime:/etc/localtime:ro - /etc/machine-id:/etc/machine-id:ro - /etc/timezone:/etc/timezone:ro # environment: # DISABLE_IPV6: \u0026#39;true\u0026#39; networks: default: external: name: ssl_proxy_network You can download this here and put it on your server.\nNow you can run docker-compose up -d and docker-compose will download the image, create the nginx/data and nginx/letsencrypt directories, and start the proxy manager. The image is roughly 700 megs, and on my test Raspberry Pi, it took a couple of minutes to download and start. Run docker-compose logs -f to see what\u0026rsquo;s happening inside the container as it does initial setup. On slower hardware, it can take over a minute to start up and get the service running, especially on the first run, so be patient.\nUsage Set up a demo container to proxy I\u0026rsquo;m going to use a simple nginx demo server to demo SSL proxying. We don\u0026rsquo;t need anything fancy, just something that will serve a web page so we can confirm the proxy is working. Make a demo directory, and put the following snippet into its docker-compose.yaml file.\nI deliberately did not set up port forwarding in this configuration file because I don\u0026rsquo;t want it to be accessible from outside our ssl_proxy_network docker network - that\u0026rsquo;s what the proxy is for.\nversion: \u0026#39;3\u0026#39; services: demo: image: nginxdemos/hello restart: unless-stopped container_name: demo volumes: - /etc/hostname:/etc/hostname:ro - /etc/localtime:/etc/localtime:ro - /etc/machine-id:/etc/machine-id:ro - /etc/timezone:/etc/timezone:ro networks: default: external: name: ssl_proxy_network Start it up with docker-compose up -d. Because you\u0026rsquo;re using the same ssl_proxy_network that you created earlier, and your nginx-proxy-manager is also using that network, the two containers will be able to communicate with each other. Open yourserver.example.com in your browser, it should give you an error message about being unable to connect. You want to see that because you don\u0026rsquo;t want the service accessible except through the proxy.\nConfigure the Proxy Server Now that you have the demo backend and the nginx-proxy-manager proxy running, let\u0026rsquo;s set up proxying.\nFirst, log into the proxy manager at https://yourserver.example.com:81. The default username is admin@example.com, and the password is changeme. It\u0026rsquo;ll make you change those when you first log in.\nAdd an SSL certificate. To make things easier later, you\u0026rsquo;re going to create a wildcard SSL certificate for your domain. This will let you run as many services as you like as servicename.example.com on the server, and not have to specify port numbers, just add new DNS entries pointing at your server.\nSelect SSL Certificates. You\u0026rsquo;ll see\nClick Add SSL certificate. You should see something like this:\nPut *.yourdomain.com in as the domain name. Select Use DNS Challenge Put in your email address so LetsEncrypt can send you notifications if there\u0026rsquo;s an issue with your certificate later. Select Route 53 for the DNS provider Set the AWS access key to the one you created for your r53-acme-user IAM user earlier Set the AWS secret key Agree to the terms of service, and hit save. This can take a few minutes, especially if your machine is low powered or the LetsEncrypt backend is under load.\nIf everything went according to plan, you should see something like this:\nAdd a proxy host Click hosts, then proxy host from the submenu.\nYou\u0026rsquo;ll see a dialog like this, hit Add Proxy Host\nYou could have several different DNS names point at the same backend, but for this example, you only want demo.yourdomain.com, so stick that in the Domain Names field. The hostname should be the name of the container we\u0026rsquo;re proxying - the demo service we set up earlier uses http on port 80, so enter those here. Even though there isn\u0026rsquo;t a port entry in the docker-compose file, the ports on the backend will be accessible from other containers on the same docker network, and this is why you configured the nginx-proxy-manager and demo services to both use the external ssl_proxy_network network. I haven\u0026rsquo;t encountered problems with any backends using them, so I turn on Cache Assets, Websockets Support and Block Common Exploits since nginx-proxy-manager can add them. Don\u0026rsquo;t hit save yet, you still need to attach an SSL certificate. Click on the SSL tab at the top of the dialog.\nSelect the *.yourdomain.com certificate you created earlier\nFinally, turn on Force SSL and HTTP/2 Support\nConfirm things are working Go to yourserver.yourdomain.com and you should see the demo page\nAnd there should be a lock icon in your browser.\nAdd authentication Some services you\u0026rsquo;re going to want to run (like our demo) don\u0026rsquo;t have any user authentication. Fortunately, nginx-proxy-manager lets you insert a basic auth login step before a connection is created to the service it\u0026rsquo;s proxying.\nClick Access Lists in the main menu, and Add Access List.\nPut in a name - I used demo-acl\nClick on Authorization\nPut in a username and password. I used demo and demo here.\nI don\u0026rsquo;t care where the service is accessed from as long as they have a valid username \u0026amp; password, so click on the Access tab, so I\u0026rsquo;m allowing from 0.0.0.0/8 - you could put in your lan\u0026rsquo;s network and netmask if you wanted to be more strict.\nSave. Go back to Hosts -\u0026gt; Proxy Hosts, and use the \u0026hellip; menu on the right of your demo proxy entry to Edit.\nSelect your new ACL and Save\nNow when you go to the service, it should prompt you with a basic auth dialog.\nLock down the proxy manager management UI Now that you have SSL proxying working, there\u0026rsquo;s one last thing to do - putting the management interface behind itself, so that it is protected by SSL too.\nIn this example, I\u0026rsquo;m running nginx-proxy-manager on cthulhu, one of the HC2s in my homelab. The container is unimaginatively named nginxproxymanager, so I set up the proxy manager to route cthulhu.miniclusters.rocks to port 81 of the nginxproxymanager as seen below.\nAfter you confirm it\u0026rsquo;s working, do a docker-compose down in your proxy manager directory, delete or comment out the - '81:81' line in docker-compose.yaml, and restart it with docker-compose up -d. If you\u0026rsquo;ve done it correctly, if you try to access port 81 on your host you\u0026rsquo;ll get a connection refused, but if you connect to it with https://yourmachine.example.com, you\u0026rsquo;ll see the admin interface come up and it\u0026rsquo;ll have a lock icon to show it\u0026rsquo;s an SSL connection.\nUpdate 2023-07-30: Add the instructions on securing the management interface with SSL that I forgot to commit.\n","permalink":"https://unixorn.github.io/post/hass/2023-07-09-set-up-nginx-proxy-manager/","summary":"\u003cp\u003eIn the next few posts, I\u0026rsquo;m going to document how to set up Home Assistant (HA) from scratch. We\u0026rsquo;re going to want to protect the admin UI interfaces for HA and its support services with SSL, and add authentication to services that don\u0026rsquo;t provide it themselves.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;re going to do this with \u003ca href=\"https://nginxproxymanager.com/\"\u003eNginx Proxy Manager\u003c/a\u003e because it has built in support for using \u003ca href=\"https://letsencrypt.org\"\u003eLetsEncrypt\u003c/a\u003e to obtain free SSL certificates, supports adding authentication to services that don\u0026rsquo;t do it themselves, and is overall easy to use.\u003c/p\u003e\n\u003cp\u003eBefore I start writing more Home Assitant articles, let\u0026rsquo;s set up a SSL proxy server to keep everything secure.\u003c/p\u003e","title":"Set up nginx-proxy-manager with LetsEncrypt SSL certificates"},{"content":"Some of my posts assume that the user already has ESPHome installed, so I\u0026rsquo;m documenting how to install it here so I don\u0026rsquo;t have to repeat it everywhere.\nInstallation This assumes you already have a machine with docker and docker-compose installed on it. If not, go install that first. You don\u0026rsquo;t need to run the ESPHome server all of the time, just when you\u0026rsquo;re configuring/adding devices.\nYou\u0026rsquo;re going to need a USB-\u0026gt;UART adapter to reflash devices - mine is a DollaTek 3.3V / 5V USB to TTL Converter CH340G UART Serial Adapter Module. If you don\u0026rsquo;t already have an adapter, make sure that the one you get works with both 3.3 volt and 5 volt devices for flexibility later.\nPrep your ESPHome directory First, make a directory to put a docker-compose.yml file in. Create a config subdirectory in it so that ESPHome has someplace to put device configuration YAML files. You\u0026rsquo;re going to want to keep those files to make it easier to update your devices later.\nCreate a docker-compose.yml file with the following contents:\nversion: \u0026#39;3\u0026#39; services: esphome: container_name: esphome environment: - TZ=America/Denver image: ghcr.io/esphome/esphome ports: - \u0026#34;6052:6052/tcp\u0026#34; volumes: - $PWD/config:/config - /etc/localtime:/etc/localtime:ro privileged: true restart: unless-stopped You\u0026rsquo;re also going to need a file to store secrets like your WIFI network name and password. Create config/secrets.yml with the following contents:\nwifi_ssid: \u0026#34;WIFI_NETWORK_TO_JOIN\u0026#34; wifi_password: \u0026#34;WIFI_PASSWORD\u0026#34; api_password: \u0026#34;AnAPIPassword\u0026#34; This will let you use secrets in your device configurations as !secretname.\nRun it I only run ESPHome when I\u0026rsquo;m adding a new device or updating an existing one, so I usually run it on my laptop. It\u0026rsquo;s more convenient this way since I don\u0026rsquo;t have to go downstairs to the server rack to connect whatever device I\u0026rsquo;m working on.\nRun docker-compose up -d on your laptop and you\u0026rsquo;ll see the ESPHome UI at http://localhost:6532.\nYou need a browser that supports Webserial to be able to flash devices directly from the ESPHome webui - on my Mac, I use Chrome for this since Firefox doesn\u0026rsquo;t support it.\n","permalink":"https://unixorn.github.io/post/hass/2023-05-21-install-esphome/","summary":"\u003cp\u003eSome of my posts assume that the user already has \u003ca href=\"https://esphome.io/\"\u003eESPHome\u003c/a\u003e installed, so I\u0026rsquo;m documenting how to install it here so I don\u0026rsquo;t have to repeat it everywhere.\u003c/p\u003e","title":"Installing ESPHome"},{"content":"As part of moving from Twitter to Mastodon I decided to add comments to the blog using Fediverse posts. Fortunately, Carl Schwan showed how he does it on his blog here.\nHere are the exact tweaks to his post I did to get it working with the papermod theme I\u0026rsquo;m using on this blog.\nBackground I\u0026rsquo;ve been meaning to add comments to this blog for a while, and migrating to mastodon from twitter made me think that there had to way to use mastodon toots as comments. I don\u0026rsquo;t have a lot of javascript experience since I only work with back-end services and don\u0026rsquo;t do any front-end work so I hadn\u0026rsquo;t gotten around to it, then I saw Carl\u0026rsquo;s post. Veronica Berglyd Olsen extended Carl\u0026rsquo;s post to make the comments display threaded on her blog.\nCarl did all the heavy lifting and Veronica extended his work. I did have to do some minor tweaks both to merge Veronica\u0026rsquo;s update and because I\u0026rsquo;m running mainline Hugo instead of a variant build with SCSS support, make everything work with raw CSS instead of SCSS .\nInstallation Carl is kind enough to publish his blog\u0026rsquo;s source in a public repository on GitLab.\nAny broken bits (like not realizing there was more to getting this to css than stripping the var statements out of it in the original version of this post) here are errors made by me when I modified Carl \u0026amp; Veronica\u0026rsquo;s code - look at their blog posts if you run into any issues.\nInstead of directly modifying the theme files in place, we\u0026rsquo;re going to override them. This will make it easier to update the theme without breaking all your comments.\nFirst, make the override directories you\u0026rsquo;re going to need with mkdir -p layouts/default layouts/partials/mastodon static/css static/assets/js at the root level of your blog repository.\nNow that the directories are in place, we\u0026rsquo;re going to make a partial with Carl\u0026rsquo;s comment code (as modified by Veronica) in it. Put this combined code in layouts/partials/mastodon/mastodon.html.\n{{ with .Params.comments }} \u0026lt;div class=\u0026#34;article-content\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Comments\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;You can use your Mastodon account to reply to this\u0026lt;a class=\u0026#34;button\u0026#34; href=\u0026#34;https://{{ .host }}/@{{ .username }}/{{ .id }}\u0026#34;\u0026gt;post\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;button class=\u0026#34;button\u0026#34; id=\u0026#34;replyButton\u0026#34; href=\u0026#34;https://{{ .host }}/@{{ .username }}/{{ .id }}\u0026#34;\u0026gt;Reply\u0026lt;/button\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;dialog id=\u0026#34;toot-reply\u0026#34; class=\u0026#34;mastodon\u0026#34; data-component=\u0026#34;dialog\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Reply to {{ .username }}\u0026#39;s post\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt; With an account on the Fediverse or Mastodon, you can respond to this post. Since Mastodon is decentralized, you can use your existing account hosted by another Mastodon server or compatible platform if you don\u0026#39;t have an account on this one. \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Copy and paste this URL into the search field of your favourite Fediverse app or the web interface of your Mastodon server.\u0026lt;/p\u0026gt; \u0026lt;div class=\u0026#34;copypaste\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; readonly=\u0026#34;\u0026#34; value=\u0026#34;https://{{ .host }}/@{{ .username }}/{{ .id }}\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;button\u0026#34; id=\u0026#34;copyButton\u0026#34;\u0026gt;Copy\u0026lt;/button\u0026gt; \u0026lt;button class=\u0026#34;button\u0026#34; id=\u0026#34;cancelButton\u0026#34;\u0026gt;Close\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/dialog\u0026gt; \u0026lt;p id=\u0026#34;mastodon-comments-list\u0026#34;\u0026gt;\u0026lt;button id=\u0026#34;load-comment\u0026#34; class=\u0026#34;button\u0026#34;\u0026gt;Load comments\u0026lt;/button\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;noscript\u0026gt;\u0026lt;p\u0026gt;You need JavaScript to view the comments.\u0026lt;/p\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;script src=\u0026#34;/assets/js/purify.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; const dialog = document.querySelector(\u0026#39;dialog\u0026#39;); document.getElementById(\u0026#39;replyButton\u0026#39;).addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { dialog.showModal(); }); document.getElementById(\u0026#39;copyButton\u0026#39;).addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { navigator.clipboard.writeText(\u0026#39;https://{{ .host }}/@{{ .username }}/{{ .id }}\u0026#39;); }); document.getElementById(\u0026#39;cancelButton\u0026#39;).addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { dialog.close(); }); dialog.addEventListener(\u0026#39;keydown\u0026#39;, e =\u0026gt; { if (e.key === \u0026#39;Escape\u0026#39;) dialog.close(); }); const dateOptions = { year: \u0026#34;numeric\u0026#34;, month: \u0026#34;numeric\u0026#34;, day: \u0026#34;numeric\u0026#34;, hour: \u0026#34;numeric\u0026#34;, minute: \u0026#34;numeric\u0026#34;, }; function escapeHtml(unsafe) { return unsafe .replace(/\u0026amp;/g, \u0026#34;\u0026amp;amp;\u0026#34;) .replace(/\u0026lt;/g, \u0026#34;\u0026amp;lt;\u0026#34;) .replace(/\u0026gt;/g, \u0026#34;\u0026amp;gt;\u0026#34;) .replace(/\u0026#34;/g, \u0026#34;\u0026amp;quot;\u0026#34;) .replace(/\u0026#39;/g, \u0026#34;\u0026amp;#039;\u0026#34;); } document.getElementById(\u0026#34;load-comment\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, function() { document.getElementById(\u0026#34;load-comment\u0026#34;).innerHTML = \u0026#34;Loading\u0026#34;; fetch(\u0026#39;https://{{ .host }}/api/v1/statuses/{{ .id }}/context\u0026#39;) .then(function(response) { return response.json(); }) .then(function(data) { if(data[\u0026#39;descendants\u0026#39;] \u0026amp;\u0026amp; Array.isArray(data[\u0026#39;descendants\u0026#39;]) \u0026amp;\u0026amp; data[\u0026#39;descendants\u0026#39;].length \u0026gt; 0) { document.getElementById(\u0026#39;mastodon-comments-list\u0026#39;).innerHTML = \u0026#34;\u0026#34;; data[\u0026#39;descendants\u0026#39;].forEach(function(reply) { reply.account.display_name = escapeHtml(reply.account.display_name); reply.account.reply_class = reply.in_reply_to_id == \u0026#34;{{ .id }}\u0026#34; ? \u0026#34;reply-original\u0026#34; : \u0026#34;reply-child\u0026#34;; reply.account.emojis.forEach(emoji =\u0026gt; { reply.account.display_name = reply.account.display_name.replace(`:${emoji.shortcode}:`, `\u0026lt;img src=\u0026#34;${escapeHtml(emoji.static_url)}\u0026#34; alt=\u0026#34;Emoji ${emoji.shortcode}\u0026#34; height=\u0026#34;20\u0026#34; width=\u0026#34;20\u0026#34; /\u0026gt;`); }); mastodonComment = `\u0026lt;div class=\u0026#34;mastodon-wrapper\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;comment-level ${reply.account.reply_class}\u0026#34;\u0026gt;\u0026lt;svg viewBox=\u0026#34;0 0 32 32\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; fill=\u0026#34;none\u0026#34; transform=\u0026#34;rotate(180)\u0026#34;\u0026gt;\u0026lt;g id=\u0026#34;SVGRepo_bgCarrier\u0026#34; stroke-width=\u0026#34;0\u0026#34;\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;g id=\u0026#34;SVGRepo_tracerCarrier\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34;\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;g id=\u0026#34;SVGRepo_iconCarrier\u0026#34;\u0026gt; \u0026lt;path stroke=\u0026#34;#535358\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34; stroke-width=\u0026#34;2\u0026#34; d=\u0026#34;M5.608 12.526l7.04-6.454C13.931 4.896 16 5.806 16 7.546V11c13 0 11 16 11 16s-4-10-11-10v3.453c0 1.74-2.069 2.65-3.351 1.475l-7.04-6.454a2 2 0 010-2.948z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt; \u0026lt;/g\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;mastodon-comment\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;avatar\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;${escapeHtml(reply.account.avatar_static)}\u0026#34; height=60 width=60 alt=\u0026#34;\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;${reply.account.url}\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt; \u0026lt;span\u0026gt;${reply.account.display_name}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;disabled\u0026#34;\u0026gt;${escapeHtml(reply.account.acct)}\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;date\u0026#34; href=\u0026#34;${reply.uri}\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt; ${reply.created_at.substr(0, 10)} \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;mastodon-comment-content\u0026#34;\u0026gt;${reply.content}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;`; document.getElementById(\u0026#39;mastodon-comments-list\u0026#39;).appendChild(DOMPurify.sanitize(mastodonComment, {\u0026#39;RETURN_DOM_FRAGMENT\u0026#39;: true})); }); } else { document.getElementById(\u0026#39;mastodon-comments-list\u0026#39;).innerHTML = \u0026#34;\u0026lt;p\u0026gt;No comments found\u0026lt;/p\u0026gt;\u0026#34;; } }); }); \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; {{ end }} If you want to change the threading icon, find another svg and replace the \u0026lt;svg\u0026gt;....\u0026lt;/svg\u0026gt; tag in the comment-level div.\nThe code needs a css file to define how the comments are going to look. I defined the background-color, padding, and border-radius inline and compiled Carl\u0026rsquo;s original scss to css with scss-to-css instead of using SCSS variables. I\u0026rsquo;m running stock hugo and didn\u0026rsquo;t want to switch to the variant supporting SCSS since I\u0026rsquo;m not using it anywhere else.\nCarl\u0026rsquo;s original SCSS code is here.\nHere\u0026rsquo;s a compiled version (with Veronica\u0026rsquo;s additions merged in) below in static/css/mastodon.css.\n.mastodon-wrapper { display: flex; gap: 3rem; flex-direction: row; } .comment-level { max-width: 3rem; min-width: 3rem; } .reply-original { display: none; } #article-comments div.reply-original { display: none; } #article-comments div.reply-child { display: block; flex: 0 0 1.75rem; text-align: right; } .mastodon-comment { background-color: #e9e5e5; border-radius: 10px; padding: 30px; margin-bottom: 1rem; display: flex; gap: 1rem; flex-direction: column; flex-grow: 2; } .mastodon-comment .comment { display: flex; flex-direction: row; gap: 1rem; flex-wrap: true; } .mastodon-comment .comment-avatar img { width: 6rem; } .mastodon-comment .content { flex-grow: 2; } .mastodon-comment .comment-author { display: flex; flex-direction: column; } .mastodon-comment .comment-author-name { font-weight: bold; } .mastodon-comment .comment-author-name a { display: flex; align-items: center; } .mastodon-comment .comment-author-date { margin-left: auto; } .mastodon-comment .disabled { color: #34495e; } .mastodon-comment-content p:first-child { margin-top: 0; } .mastodon { --dlg-bg: #282c37; --dlg-w: 600px; --dlg-color: #9baec8; --dlg-button-p: 0.75em 2em; --dlg-outline-c: #00D9F5; } .copypaste { display: flex; align-items: center; gap: 10px; } .copypaste input { display: block; font-family: inherit; background: #17191f; border: 1px solid #8c8dff; color: #9baec8; border-radius: 4px; padding: 6px 9px; line-height: 22px; font-size: 14px; transition: border-color 0.3s linear; flex: 1 1 auto; overflow: hidden; } .copypaste .button { border: 10px; border-radius: 4px; box-sizing: border-box; color: #fff; cursor: pointer; display: inline-block; font-family: inherit; font-size: 15px; font-weight: 500; letter-spacing: 0; line-height: 22px; overflow: hidden; padding: 7px 18px; position: relative; text-align: center; text-decoration: none; text-overflow: ellipsis; white-space: nowrap; width: auto; background-color: #232730; } .copypaste .button:hover { background-color: #16181e; } Update: Stewart Wright posted an article based on this, but his css works in dark mode too - add this to mastodon.css to enable dark mode.\n.dark .mastodon-comment { background-color: #36383d; } .dark .mastodon-comment .disabled { color: #ad55fd; } It also needs a local copy of DOMPurify - curl https://raw.githubusercontent.com/cure53/DOMPurify/main/dist/purify.min.js \u0026gt; static/assets/js/purify.min.js\nFinally, create layouts/default/single.html\nFirst we need to add a link to the style sheet we added - I added a link to the stylesheet at the beginning \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; type=\u0026quot;text/css\u0026quot; href=\u0026quot;{{.Site.BaseURL}}css/mastodon.css\u0026quot; /\u0026gt;\nWe also need to add the mastodon.html partial file to present the Load Commants and Reply buttons.\nRight after the post-content div, I added\n\u0026lt;div\u0026gt; {{ partial \u0026#34;mastodon/mastodon.html\u0026#34; .}} \u0026lt;/div\u0026gt; Here\u0026rsquo;s the full version of my modified copy of the papermod layout file so you don\u0026rsquo;t have to edit it yourself.\n{{- define \u0026#34;main\u0026#34; }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;{{.Site.BaseURL}}css/mastodon.css\u0026#34; /\u0026gt; \u0026lt;article class=\u0026#34;post-single\u0026#34;\u0026gt; \u0026lt;header class=\u0026#34;post-header\u0026#34;\u0026gt; {{ partial \u0026#34;breadcrumbs.html\u0026#34; . }} \u0026lt;h1 class=\u0026#34;post-title\u0026#34;\u0026gt; {{ .Title }} {{- if .Draft }}\u0026lt;sup\u0026gt;\u0026lt;span class=\u0026#34;entry-isdraft\u0026#34;\u0026gt;\u0026amp;nbsp;\u0026amp;nbsp;[draft]\u0026lt;/span\u0026gt;\u0026lt;/sup\u0026gt;{{- end }} \u0026lt;/h1\u0026gt; {{- if .Description }} \u0026lt;div class=\u0026#34;post-description\u0026#34;\u0026gt; {{ .Description }} \u0026lt;/div\u0026gt; {{- end }} {{- if not (.Param \u0026#34;hideMeta\u0026#34;) }} \u0026lt;div class=\u0026#34;post-meta\u0026#34;\u0026gt; {{- partial \u0026#34;post_meta.html\u0026#34; . -}} {{- partial \u0026#34;translation_list.html\u0026#34; . -}} {{- partial \u0026#34;edit_post.html\u0026#34; . -}} {{- partial \u0026#34;post_canonical.html\u0026#34; . -}} \u0026lt;/div\u0026gt; {{- end }} \u0026lt;/header\u0026gt; {{- $isHidden := .Params.cover.hidden | default site.Params.cover.hiddenInSingle | default site.Params.cover.hidden }} {{- partial \u0026#34;cover.html\u0026#34; (dict \u0026#34;cxt\u0026#34; . \u0026#34;IsHome\u0026#34; false \u0026#34;isHidden\u0026#34; $isHidden) }} {{- if (.Param \u0026#34;ShowToc\u0026#34;) }} {{- partial \u0026#34;toc.html\u0026#34; . }} {{- end }} {{- if .Content }} \u0026lt;div class=\u0026#34;post-content\u0026#34;\u0026gt; {{- if not (.Param \u0026#34;disableAnchoredHeadings\u0026#34;) }} {{- partial \u0026#34;anchored_headings.html\u0026#34; .Content -}} {{- else }}{{ .Content }}{{ end }} \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; {{ partial \u0026#34;mastodon/mastodon.html\u0026#34; .}} \u0026lt;/div\u0026gt; {{- end }} \u0026lt;footer class=\u0026#34;post-footer\u0026#34;\u0026gt; {{- $tags := .Language.Params.Taxonomies.tag | default \u0026#34;tags\u0026#34; }} \u0026lt;ul class=\u0026#34;post-tags\u0026#34;\u0026gt; {{- range ($.GetTerms $tags) }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .LinkTitle }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{- end }} \u0026lt;/ul\u0026gt; {{- if (.Param \u0026#34;ShowPostNavLinks\u0026#34;) }} {{- partial \u0026#34;post_nav_links.html\u0026#34; . }} {{- end }} {{- if (and site.Params.ShowShareButtons (ne .Params.disableShare true)) }} {{- partial \u0026#34;share_icons.html\u0026#34; . -}} {{- end }} \u0026lt;/footer\u0026gt; {{- if (.Param \u0026#34;comments\u0026#34;) }} {{- partial \u0026#34;comments.html\u0026#34; . }} {{- end }} \u0026lt;/article\u0026gt; {{- end }}{{/* end main */}} Usage Now that all the files are in place, all you have to do to add comments to a post is add a comments stanza to the front matter in your post file to let the comment load know what toot to look at.\nHere\u0026rsquo;s an example of the comments stanza for this post:\ncomments: host: hachyderm.io username: unixorn id: 110149495764332469 The only awkward bit is that first you need to create a toot pointing at your blog post, then get the id, then update the post front matter to include the comment section.\nUpdates 2023-03-06 - Compiled the SCSS to CSS. Thanks for the catch, Carl. 2023-03-06 - Added indented comments based on Veronica Berglyd Olsen\u0026rsquo;s post 2023-04-24 - Fix typo in file paths - /partial/ should have been /partials/ - thanks for the catch, Stewart. Also updated mastodon.css to include Stewart\u0026rsquo;s dark css section to make the comments visible when the blog is in dark mode. ","permalink":"https://unixorn.github.io/post/2023-03-blog-comments-via-mastodon/","summary":"\u003cp\u003eAs part of moving from Twitter to Mastodon I decided to add comments to the blog using Fediverse posts. Fortunately, Carl Schwan showed how he does it on his blog \u003ca href=\"https://carlschwan.eu/2020/12/29/adding-comments-to-your-static-blog-with-mastodon/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere are the exact tweaks to his post I did to get it working with the \u003ca href=\"https://github.com/adityatelange/hugo-PaperMod\"\u003epapermod\u003c/a\u003e theme I\u0026rsquo;m using on this blog.\u003c/p\u003e","title":"Blog Comments via Mastodon"},{"content":"Just got an Orange Pi 5. I couldn\u0026rsquo;t find a simple set of instructions on how to boot it off the M.2 NVMe slot, so I\u0026rsquo;m documenting it here.\nBackground I didn\u0026rsquo;t find simple instructions for getting the Orange Pi 5 to boot off of NVMe. It\u0026rsquo;s not difficult, just not documented in one place that I could find.\nParts List An Orange Pi 5. I got a 16GB one from Amazon for $149 A microSD card. I used a Sandisk 32GB card because they\u0026rsquo;re reliable, quick, and cost effective. You don\u0026rsquo;t need something that fast or reliable, you\u0026rsquo;re only going to use it long enough for imaging. An NVMe drive. If you want it to fit neatly without sticking out from under the Orange Pi, you need a 2230 or 2242 form factor card. I got a SAMSUNG 512GB M.2 2242 model MZALQ512HALU) for $49. Unfortunately the Orange Pi 5 doesn\u0026rsquo;t come with an M.2 mounting screw, so I bought a bag full from Amazon for $5.97 Instructions Preparatiion Download Joshua Riek\u0026rsquo;s ubuntu build from Joshua-Riek/ubuntu-orange-pi5. Here\u0026rsquo;s a direct link to the releases. I used Burn it to a microSD card. I use Balena Ether because I can use it on my Mac or Linux boxes, and it validates the results automatically for you. Install the NVMe drive. Update the SPI firmware Boot with the microSD card. I didn\u0026rsquo;t bother to do any more configuration than it forced me to since I knew I was going to have to do it again after switching to the NVMe drive Flash the SPI bootloader by running sudo dd if=/lib/u-boot-orangepi-rk3588/rkspi_loader.img of=/dev/mtdblock0 conv=notrunc Prep the NVMe drive Run sudo lsblk to see what devices the drives on your machine are recognized as. It should be easy to tell them apart by their size. On my machine, the NVMe drive was /dev/nvme0n1 and my microSD card was /dev/mmcblk1. This step will nuke any data on the NVMe drive and it will be difficult to retrieve the data if it\u0026rsquo;s even possible at all. sudo cat /dev/mmcblk1 \u0026gt; /dev/nvme0n1. It won\u0026rsquo;t print anything, and may take 20-30 minutes depending on the size of your drive. Be patient. Once you\u0026rsquo;re done, shut down the system with sudo shutdown -h now, remove the microSD card and reboot.\nThe first boot from NVMe may take a few minutes - Joshua\u0026rsquo;s ubuntu build is automagically extending the filesystem to consume the entire NVMe drive.\nBenchmarks I ran the storage benchmark from PiBenchmarks.\nCategory Test Result HDParm Disk Read 367.77 MB/s HDParm Cached Disk Read 362.99 MB/s DD Disk Write 232 MB/s FIO 4k random read 87521 IOPS (350085 KB/s) FIO 4k random write 40235 IOPS (160943 KB/s) IOZone 4k read 72318 KB/s IOZone 4k write 105149 KB/s IOZone 4k random read 38021 KB/s IOZone 4k random write 69923 KB/s Score: 20187 For comparison, when I ran it on my Odroid N2 with a USB 3.0 SSD, the Odroid wasn\u0026rsquo;t just embarrassed, it was humiliated. M.2 for the win!\nCategory Test Result HDParm Disk Read 237.62 MB/s HDParm Cached Disk Read 133.27 MB/s DD Disk Write 20.4 MB/s FIO 4k random read 5017 IOPS (20068 KB/s) FIO 4k random write 2258 IOPS (9034 KB/s) IOZone 4k read 36002 KB/s IOZone 4k write 8283 KB/s IOZone 4k random read 14312 KB/s IOZone 4k random write 7929 KB/s Score: 2443 Update: For the curious, here\u0026rsquo;s the uname output: Linux medusa.example.com 5.10.110-rockchip-rk3588 #1 SMP Wed Mar 29 08:28:12 BST 2023 aarch64 aarch64 aarch64 GNU/Linux\n","permalink":"https://unixorn.github.io/post/2023-03-boot-orange-pi-5-from-nvme/","summary":"\u003cp\u003eJust got an Orange Pi 5. I couldn\u0026rsquo;t find a simple set of instructions on how to boot it off the M.2 NVMe slot, so I\u0026rsquo;m documenting it here.\u003c/p\u003e","title":"Booting an Orange Pi 5 from NVMe"},{"content":"We\u0026rsquo;re doing a kitchen \u0026amp; bathrooms renovation, and the construction is generating a lot of dust, to the point that I was changing the HVAC filter twice a week. I wanted to see just how much garbage is in the air, so I started looking around for air quality sensors. Naturally I wanted one that I could integrate into my Home Assistant so I could generate notifications if the air was extra filthy.\nMost of the sensors I found were either over priced, didn\u0026rsquo;t expose their data via a local API, or both.\nIkea has a very cheap ($15 US) VINDRIKTNING sensor that measures 2.5 µm particles in the air, and after looking around online, I realized that I could splice an ESP8266 or ESP32 into it and transform it from something that just displays relatively useless green, red \u0026amp; orange LED lights to something I could scrape more meaningful data into my Home Assistant installation and potentially trigger automations.\nI decided to use Tasmota because it already has support for the VINDRIKTNING\u0026rsquo;s air sensor baked into the all-sensors version of the firmware, and also supports i2c so that in addition to the VINDRIKTNING\u0026rsquo;s 2.5 µm sensor, I added a BME680 from my parts box and get temperature, humidity, dew point and VOC measurements too.\nParts List Small Phillips screwdriver Wemos D1 Mini - Any ESP32 or ESP8266 will do, this is what I had in my parts bin Ikea VINDRIKTNING air sensor USB C cable \u0026amp; power brick to power the VINDRIKTNING, it doesn\u0026rsquo;t come with them BME688 (optional) qwiic cable for cannibalization (optional) Pre-requisites A MQTT server Home Assistant configured to use MQTT Software Setup First, flash the D1 - it\u0026rsquo;s easier to do when it isn\u0026rsquo;t wired into the Ikea unit. I used Tasmota\u0026rsquo;s online tool to flash my board and chose the Tasmota all-sensors build - make sure you use all-sensors, not the sensors build at the top of the popup menu or it won\u0026rsquo;t have the VINDRIKTNING support baked in.\nNext, make sure it\u0026rsquo;s connecting to your WIFI and you can reach the web UI panel. Get this working now so that you don\u0026rsquo;t have to open up the case later to reflash it. You want to be sure the board is working before you do all the soldering to embed it into the VINDRIKTNING.\nConfiguration Tasmota defaults to sending metrics to MQTT every 600 seconds. Given that the VINDRIKTNING is powered by a USB brick and I don\u0026rsquo;t have to care about battery life, I set mine to report every 60 seconds so my graphs will be smoother. To do that, connect to the Tasmota\u0026rsquo;s web ui, select the console, and enter TelePeriod 60 (or whatever interval in seconds you prefer).\nSet up MQTT Go to the web ui and go into the configuration menu, then MQTT. The topic you specify will be the device name in Home Assistant.\nSet up pins Next, go into the configuration menu and configure what pins are going to be used for what. Here\u0026rsquo;s what my working setup looks like:\nHardware Setup We\u0026rsquo;re not going to replace the Ikea\u0026rsquo;s electronics. Instead, we\u0026rsquo;re going to tap into it like a symbiote so that the VINDRIKTNING supplies power and data to our 8266, and the original board remains fully functional to drive the LED display.\nSteps Open up the VINDRIKTNING Unfortunately the screws holding it together are in annoyingly deep holes, so you\u0026rsquo;ll need a eyeglass-style screwdriver like this one that came with a fan kit for one of my Raspberry Pis (AirPods Pro for scale).\nOpen it gently - the screws are tapped into the plastic case and it is easy to strip the holes.\nOnce it\u0026rsquo;s open, carefully unsnap the two cables connecting the sensor and the motherboard. It\u0026rsquo;ll look like this when you\u0026rsquo;re done:\nI didn\u0026rsquo;t bother to unscrew the IKEA motherboard from the case, there aren\u0026rsquo;t that many connections to make to it and they\u0026rsquo;re pretty easily accessible.\nWire up the ESP8266 Connect a QWIIC cable to the Wemos I2C Bus (Optional) If you\u0026rsquo;re not going to use the i2c bus to connect another sensor, you can skip this step.\nI happened to have a BME688 in my parts bin that I could add to the sensor, so I cut a QWIIC cable in half and soldered that onto the Wemos before I connected that to the Ikea.\nConnect the wires as follows:\nRed wire -\u0026gt; 3v3 Black wire -\u0026gt; Ground G Yellow -\u0026gt; D1 Blue -\u0026gt; D2 Here\u0026rsquo;s a picture of the D1 with the QWIIC cable - ignore the green wire, I forgot to take a picture before starting the next step\nConnect the ESP 8266 to the VINDRIKTNING You only need to connect 3 wires for this.\nConnect D5 on the D1 to REST on the VINDRIKTNING (Blue wire in pic) Connect G on the D1 to GND on the VINDRIKTNING (Green wire in pic) Connect 5V on the D1 to +5V on the VINDRIKTNING (Red wire in pic) Now you can reconnect the cables from the VINDRIKTNING sensor to its motherboard, and it should look very similar to this image.\nReassembly Plug in the USB-C power cable and make sure that you can access the Tasmota web ui and that it is reading the sensors before you put any screws back in.\nIf you have a BME688 in yours, it\u0026rsquo;ll look like:\notherwise it\u0026rsquo;ll only have the VINDRIKTNING reading.\nNow you can carefully put the D1 (and the BME 688 if you added one) into the empty space in the case, then screw everything shut.\nConnecting to Home Assistant If you don\u0026rsquo;t already have the Tasmota integration, you\u0026rsquo;ll need to add it. Go into Settings, Devices \u0026amp; Services, then Integrations. Click Add Integration, pick the Tasmota Integration and configure it. It\u0026rsquo;ll ask for the MQTT server, user and password.\nHome Assistant should detect your new sensor and let you add the device.\nCongratulations, you\u0026rsquo;ve successfully DIYed an air sensor!\n","permalink":"https://unixorn.github.io/post/add-smarts-to-ikea-vindriktning-air-sensors/","summary":"\u003cp\u003eWe\u0026rsquo;re doing a kitchen \u0026amp; bathrooms renovation, and the construction is generating a lot of dust, to the point that I was changing the HVAC filter twice a week. I wanted to see just how much garbage is in the air, so I started looking around for air quality sensors. Naturally I wanted one that I could integrate into my Home Assistant so I could generate notifications if the air was extra filthy.\u003c/p\u003e","title":"Add Smarts to IKEA Vindriktning Sensors"},{"content":"Released version 0.8.0 of ha-mqtt-discoverable and version 0.2.0 of ha-mqtt-discoverable-cli today.\nha-mqtt-discoverable is a python module that allows python programs to create sensor and device entities on an mqtt server that will be automagically recognized by Home Assistant.\nha-mqtt-discoverable-cli installs command line scripts for device and sensor creation.\nThere is also a docker image, unixorn/ha-mqtt-discoverable-cli, that contains the command line tools.\n","permalink":"https://unixorn.github.io/post/ha-mqtt-discoverable-0.8.0/","summary":"\u003cp\u003eReleased version 0.8.0 of ha-mqtt-discoverable and version 0.2.0 of ha-mqtt-discoverable-cli today.\u003c/p\u003e","title":"Released ha-mqtt-discoverable 0.8.0"},{"content":"I\u0026rsquo;ve been backing my homelab up with duplicacy (See Backing Up the Cluster Using Duplicacy), but I\u0026rsquo;m fed up with it returning a 0 exit code even if there\u0026rsquo;s a problem with the backup. This makes me have to do a lot of annoying rummaging through log output to be sure that a backup actually worked, so I decided to switch to restic.\nIn this blog entry, I\u0026rsquo;m going to explain how to create a jail in TrueNAS, mount directories you want to back up into the jail, install restic, and how to use it to back up to Backblaze b2.\nBackground I\u0026rsquo;m tired of worrying about whether I found all the possible duplicacy error messages, or made a mistake in one of the regexes for the errors I have seen. I want my backups nice and boring so I don\u0026rsquo;t go to do a restore and find out \u0026ldquo;Oops! We had a new failure mode you hadn\u0026rsquo;t seen before, so the backups haven\u0026rsquo;t been working for three months, too bad about your data!\u0026rdquo;\nMy work had a holiday shutdown, so I decided it was past time to switch to a better backup system. Of course I\u0026rsquo;m going to keep running the old crappy one for a couple of months in parallel because I don\u0026rsquo;t want to find a problem with the new one after three months of usage when I do my next restore test. You do test your restores periodically, I hope.\nI\u0026rsquo;ve been running TrueNAS Core (formerly FreeNAS) long enough that I\u0026rsquo;ve upgraded to larger drives in the zpool three times. I picked it because FreeNAS fully supported ZFS, I could buy a chassis with 4 hot-swap bays, I could buy it from the company that makes the software, both to support the open source project and to ensure that I had fully supported hardware.\nCan I build a server from parts? Sure, I\u0026rsquo;ve done it before. Do I want to build a server from parts and have to fight with a bunch of different vendors if something goes awry? Oh hell no, especially for the server with all my photos on it. I opted to minimize my hassle factor, support the FreeNAS project, and only have to deal with one vendor in case of any hardware issues. I bought a 4 bay FreeNAS mini, and have been happy with it for 7+ years.\nAnyway, enough background on the server I\u0026rsquo;m backing up.\nWhy restic? restic is:\nOpen Source Works on macOS, Windows, Linux and (most importantly for TrueNAS) FreeBSD Written in go, so it\u0026rsquo;s a single binary blob and we don\u0026rsquo;t have to worry about installing a bunch of dependencies inside the jail Does data deduplication and compression on your backups. Since we pay Backblaze based on space usage, this reduces the backup cost Why Jails? TrueNAS Core is based on FreeBSD, so the native way of running software in an isolated environment are jails (one of the inspirations for docker containers). I could run it in a docker container, but that would burn more resources because I\u0026rsquo;d have to run it in a docker vm.\nWhy B2? I\u0026rsquo;ve been using Backblaze b2 for years. It has a s3-compatible API so a lot of s3-aware applications work with it easily, Backblaze only does storage so I don\u0026rsquo;t have to worry about it going away, and last but definitely not least, it only costs 20% of what the same amount of data would cost to store in S3.\nrestic supports a lot of other storage back ends (including local disk), but I\u0026rsquo;m only going to discuss b2 here.\nAll that said, let\u0026rsquo;s get down to it.\nSet up B2 First, create an account at Backblaze.com.\nNow that you have an account, you\u0026rsquo;ll need to create a bucket to store your backups in. Log into your account and click Buckets in the sidebar on the left, then Create a Bucket. Give your new bucket a unique name, make sure it\u0026rsquo;s set to private (which should be set by default), and click create.\nYou\u0026rsquo;ll also need an application key that restic can use to connect to b2.\nDo not use your master application key! You should restrict restic to just the bucket you want it using - don\u0026rsquo;t give it power over your entire backblaze account. Click App Keys at the bottom of the sidebar, then Add a New Application Key, give it a name (I used restic-key), select the bucket you just created from the Allow access to buckets dropdown menu, and create the key. Copy the key into your password manager, it\u0026rsquo;ll only be displayed once. Copy the keyID too, you\u0026rsquo;ll need both of them when we configure restic.\nCreate a jail for restic Use the TrueNAS wizard to create your restic jail.\nLog into your TrueNAS webui Click Jails. Click add. You should see something similar to Give it a name (make life easy for yourself and don\u0026rsquo;t include spaces or any special characters other than -), leave the jail type as default, select whatever the highest FreeBSD version is showing available in the release drop-down menu, then next It\u0026rsquo;s going to need network access, so click DHCP Autoconfigure, and if it doesn\u0026rsquo;t automatically populate the VNET checkbox, enable that too, then next It\u0026rsquo;ll show you a summary of your settings that should look something like click submit to create the new jail. Add Mount Points By default, your jail can\u0026rsquo;t see any directory trees outside itself. That\u0026rsquo;s great for security, but not so great if we want to back up files outside the jail, so we\u0026rsquo;re going to add some mount points.\nAdd the mount points before you start the new jail - you can\u0026rsquo;t add them to a running jail. You can stop the jail, add/remove mount points, then restart it, but it\u0026rsquo;s easier to set them up before starting the jail.\nSelect your new jail, and select add mountpoint .\nYou can either make individual mounts for the directories you want to back up, or you can make one for your whole zpool. Whichever you pick, I recommend setting them read-only so you can\u0026rsquo;t accidentally restore an older version of a file over the current version.\nMake a directory outside the jail to store configuration files and to restore files to, and don\u0026rsquo;t set it read-only. Segregating the config/restore directory to a separate mount point will make it easier to examine restored files before putting them back where they belong.\nNow that you have your mount points added to your jail, go ahead and start your jail. It should take less than a minute to start.\nInstall restic Now that the jail is running, you\u0026rsquo;ll have to install restic into it.\nssh into your TrueNAS server Enter the jail so you can install software by running sudo iocage console YOURJAILNAME pkg install ca_root_nss restic Configure restic \u0026amp; do your first backup For convenience, I store the various settings as environment variable exports in a shell script that I can source for a couple of reasons:\nThis makes it easy to do the backups via cron and also to do test backups and restores from within the jail. I don\u0026rsquo;t have to write a config file parser and overcomplicate things Install the driver script and config files ssh into your TrueNAS server and stick the following files in a directory visible inside your backups jail.\nHere\u0026rsquo;s the restic-driver script I use to run restic and trim snapshots once they age out.\n#!/usr/bin/env bash # # restic-driver-script # # License: Apache 2.0 # Copyright 2023 Joe Block \u0026lt;jpb@unixorn.net\u0026gt; set -o pipefail if [[ -n \u0026#34;$DEBUG\u0026#34; ]]; then set -x fi function debug() { if [[ -n \u0026#34;$DEBUG\u0026#34; ]]; then echo \u0026#34;$@\u0026#34; fi } function fail() { printf \u0026#39;%s\\n\u0026#39; \u0026#34;$1\u0026#34; \u0026gt;\u0026amp;2 ## Send message to stderr. Exclude \u0026gt;\u0026amp;2 if you don\u0026#39;t want it that way. exit \u0026#34;${2-1}\u0026#34; ## Return a code specified by $2 or 1 by default. } function has() { # Check if a command is in $PATH which \u0026#34;$@\u0026#34; \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 } function show_params() { debug \u0026#34;BACKUP_PATHS: $BACKUP_PATHS\u0026#34; debug \u0026#34;EXCLUDE_FILE: $EXCLUDE_FILE\u0026#34; debug \u0026#34;DRY_RUN: $DRY_RUN\u0026#34; debug \u0026#34; \u0026#34; debug \u0026#34;Retention settings:\u0026#34; debug \u0026#34;Minimum snapshots $MINIMUM_SNAPSHOTS_RETAINED\u0026#34; debug \u0026#34;Hourly snapshots: $HOURS_RETAINED\u0026#34; debug \u0026#34;Daily snapshots: $DAYS_RETAINED\u0026#34; debug \u0026#34;Weekly snapshots: $WEEKS_RETAINED\u0026#34; debug \u0026#34;Monthly snapshots: $MONTHS_RETAINED\u0026#34; debug \u0026#34;Yearly snapshots: $YEARS_RETAINED\u0026#34; } if ! has restic; then fail \u0026#34;Can\u0026#39;t find restic in $PATH!\u0026#34; fi # Our first argument is the settings file to source to get our backup # parameters, so peel it off - we\u0026#39;ll pass all the other arguments directly # to restic PREFS_F=\u0026#34;$1\u0026#34; shift if [[ ! -r \u0026#34;$PREFS_F\u0026#34; ]]; then fail \u0026#34;Can\u0026#39;t load $PREFS_F\u0026#34; fi source \u0026#34;$PREFS_F\u0026#34; show_params # If you\u0026#39;re backing up a filesystem that you\u0026#39;re mounting by FUSE, the inode # information is misleading at best, so add --ignore-inode. restic backup --verbose=2 \\ --exclude=.duplicacy \\ --exclude=.DS_Store \\ --tag periodic \\ -o b2.connections=15 \\ $EXCLUDE_FILE $DRY_RUN $BACKUP_PATHS $@ if [[ $? != 0 ]]; then fail \u0026#34;restic backup failed\u0026#34; # We don\u0026#39;t want to prune any snapshots if this backup failed fi # Prune backup snapshots restic forget --verbose \\ --tag periodic \\ --group-by \u0026#34;paths,tags\u0026#34; \\ --keep-last $MINIMUM_SNAPSHOTS_RETAINED \\ --keep-hourly $HOURS_RETAINED \\ --keep-daily $DAYS_RETAINED \\ --keep-weekly $WEEKS_RETAINED \\ --keep-monthly $MONTHS_RETAINED \\ --keep-yearly $YEARS_RETAINED if [[ $? != 0 ]]; then fail \u0026#34;restic snapshot cleanup failed\u0026#34; fi Here\u0026rsquo;s an example settings file for it:\n#!/usr/bin/env bash # # Here are our backup settings # export B2_ACCOUNT_ID=\u0026#39;your_b2_account_id\u0026#39; export B2_ACCOUNT_KEY=\u0026#39;your_b2_key\u0026#39; # Use different directory prefixes for each backup repo # so they can share a bucket without interference export RESTIC_REPOSITORY=\u0026#39;b2:your-restic-backups-bucket:dir-prefix\u0026#39; # This is used as the encryption key for your backups. If you lose it, # you won\u0026#39;t be able to restore anything. I keep a copy of mine in my # 1Password vault export RESTIC_PASSWORD=\u0026#39;your-encryption-key\u0026#39; # If you want to exclude some directories from your backups, list # them in an exclude file and set EXCLUDE_FILE export EXCLUDE_FILE=\u0026#34;--exclude-file=example-excludes\u0026#34; # Uncomment if you only want to dry-run and not actually write any data # to the backup repository #export DRYRUN=\u0026#39;--dry-run\u0026#39; # What paths do we want to back up? Remember to use the paths as seen inside # the jail, not the paths as seen in your TrueNAS environment export BACKUP_PATHS=\u0026#34;/mnt/path-inside-jail/share /mnt/path-inside-jail/anothershare\u0026#34; # How many snapshots do we want to keep around? export MINIMUM_SNAPSHOTS_RETAINED=4 export HOURS_RETAINED=48 export DAYS_RETAINED=14 export WEEKS_RETAINED=8 export MONTHS_RETAINED=12 export YEARS_RETAINED=5 And here\u0026rsquo;s an example excludes file - it\u0026rsquo;s a list of paths to directories and files we don\u0026rsquo;t want to back up - I don\u0026rsquo;t back up my downloads or installers directories since that\u0026rsquo;s all stuff I can re-download later if necessary, and I\u0026rsquo;m better off getting the current version anyway.\n/mnt/path-inside-jail/Downloads /mnt/path-inside-jail/Installers/*.dmg Test your backups You\u0026rsquo;ll have to do this from inside the backups jail you created. ssh to your server, then run sudo iocage console YOURJAILNAME\nI recommend you set BACKUPS_PATH in your settings file to a single small directory to make your testing faster. You can change it to the path to your full directory tree once you confirm your backups and restores are working as expected.\nInitialize your backup repository Now that you\u0026rsquo;re inside the jail, you\u0026rsquo;re going to have to initialize the repository directory in your B2 bucket. FreeBSD/TrueNAS uses csh as the default root shell, but our settings file is set up for bash, so start by running exec bash to get a decent shell.\nLoad the configuration file you created by running source /path/to/yoursettingsfile.sh, then run restic init.\nYou\u0026rsquo;re ready to do a backup.\nRun a backup Now that your repository has been initialized, run /path/to/restic-driver /path/to/yoursettingsfile.sh\nIt shouldn\u0026rsquo;t take long if you used a small test subdirectory.\nTest a restore First, let\u0026rsquo;s look at the list of snapshots with restic snapshots.\nNow we can list the files in the snapshot with either restic ls SNAPSHOTID or restic ls latest\nPick a file and restore it.\nMake a directory to restore to - mkdir ./restores Restore the file. We don\u0026rsquo;t want to restore the entire snapshot, just a specific file/directory, so we\u0026rsquo;ll use --include and run restic restore latest --target ./restores --include /mnt/path/to/file It put the whole directory path for that file inside ./restores, so we\u0026rsquo;ll do a simple md5 check with md5sum /path/to/test/file ./restores/path/to/test/file Run restic out of cron Great, you\u0026rsquo;re almost done. All we have to do now is add it to cron so it runs at least once a day. You need to add it to the crontab inside the jail though, or TrueNAS won\u0026rsquo;t find your script.\nI stored my restic backup script and configuration in a directory that appears as /mnt/alcatraz inside my backups jail. Change the path to match wherever you stored yours.\nRun crontab -e and add the following:\n# minute\thour\tmday\tmonth\twday\tcommand 13 */4 * * * /mnt/alcatraz/restic-driver /mnt/alcatraz/backup-settings.sh | logger -t backups I tagged all the log output with backups to make it easier to grep out of /var/log/messages, and I run mine four times a day. You may prefer a different cadence, if so set the hour field accordingly. restic locks the repository when it starts, so you don\u0026rsquo;t have to worry about repository corruption if you accidentally try to run more than one backup at a time, or one runs so long that it isn\u0026rsquo;t complete before the next run starts.\nUpdate: Jesse Alter wrote a perl version of this script, you can find it on GitHub at jessealter/restickit.\n","permalink":"https://unixorn.github.io/post/restic-backups-on-truenas/","summary":"I\u0026rsquo;ve been backing my homelab up with duplicacy (See Backing Up the Cluster Using Duplicacy), but I\u0026rsquo;m fed up with it returning a 0 exit code even if there\u0026rsquo;s a problem with the backup. This makes me have to do a lot of annoying rummaging through log output to be sure that a backup actually worked, so I decided to switch to restic.\nIn this blog entry, I\u0026rsquo;m going to explain how to create a jail in TrueNAS, mount directories you want to back up into the jail, install restic, and how to use it to back up to Backblaze b2.","title":"Restic Backups on TrueNAS"},{"content":"I considered a switch to the Congo theme for Hugo. The one thing I didn\u0026rsquo;t like in it was the way it handles inline backticks - it displays them, in addition to formatting the enclosed text as code.\nTo fix it, create assets/css/custom.css with the following contents:\n.prose code::before { content: \u0026#39;\u0026#39; } .prose code::after { content: \u0026#39;\u0026#39; } ","permalink":"https://unixorn.github.io/post/fix-congo-backticks/","summary":"I considered a switch to the Congo theme for Hugo. The one thing I didn\u0026rsquo;t like in it was the way it handles inline backticks - it displays them, in addition to formatting the enclosed text as code.\nTo fix it, create assets/css/custom.css with the following contents:\n.prose code::before { content: \u0026#39;\u0026#39; } .prose code::after { content: \u0026#39;\u0026#39; } ","title":"Fix Congo Backticks"},{"content":"I\u0026rsquo;m moving from Twitter to Mastodon. Specifically, I\u0026rsquo;m @unixorn@hachyderm.io. Here are my first impressions.\nIt\u0026rsquo;s refreshing to use social media without the distorting effects caused by selling ads. No algorithm tuning to keep you doomscrolling, you just see posts in reverse chronological order, from the people and hashtags you follow. No promoted posts, no \u0026ldquo;here\u0026rsquo;s a tweet that other people like, so we\u0026rsquo;re going to jam it into your feed ahead of content from accounts you\u0026rsquo;re actually following\u0026rdquo;, just posts from your follows, and all the posts from your follows, in reverse chronological order.\nAnd when you\u0026rsquo;re done reading all that\u0026rsquo;s new, it doesn\u0026rsquo;t puke up posts selected by the algorithm to make you stick around, you can close it and come back in a few hours and see only new content you care about. It reminds me a lot of USENET that way.\nGetting Started Because the fediverse is composed of many interconnected servers, you don\u0026rsquo;t have to put your account on the big popular ones that are currently being swanped with new users. And you shouldn\u0026rsquo;t. There\u0026rsquo;s no disadvantage to using different servers, you can still follow and interact with people in the rest of the fediverse. Think of it like email - if you\u0026rsquo;re on gmail, you can still send and recieve mail from people on other services like O365, on Mastodon you can post on one server and have those seen by your followers on other servers.\nJoinMastodon.org has a list of servers sorted by topics and locations. You\u0026rsquo;re better off getting an account on a smaller server that aligns with your interests so the local feed will be more interesting to you and help you find similar-minded people to follow. And smaller servers are generally faster.\nNorms Mastodon isn\u0026rsquo;t just a Twitter clone. It has been around for 5+ years, and the culture is different in a good way. Here are some of the norms - I\u0026rsquo;ve only been on Mastodon a few days and won\u0026rsquo;t pretend to have a full grasp of it.\nIn no particular order\nThere are no ads on Mastodon. Which is awesome, but it means no ad revenue. Consider donating to your server via patreon or paypal. There\u0026rsquo;s no algorithm prioritizing tweets. Favoriting something shows appreciation to the poster, but it doesn\u0026rsquo;t make their post more visible to others. If you want to help other people see it you have to boost (the equivalent of a retweet) it. DMs are not end to end encrypted. Admins can read them if necessary. If you want to really privately talk, use Signal Mastodon is far more into letting people consent to read your content over letting you just jam it into your followers\u0026rsquo; eyeballs. If something is at all likely to disturb a reader, use the content warning. It will let you mention why there\u0026rsquo;s a content warning. You can also use it for media spoilers. Unlike the bird site, Mastodon doesn\u0026rsquo;t do whole-toot searches of toots, only hashtag ones. Partly because it\u0026rsquo;s computationally expensive, but mostly to prevent pile-ons by assholes searching for posts with a word like LGBT to find people to abuse. Because of this, hashtagging your posts is crucial if you want non-followers to see them. Be screen-reader friendly and camel cap your hashtags - most screenreaders will use that to properly pronounce them, so use#HomeAssistant instead of #homeassistant. While I\u0026rsquo;m talking about screen readers, please use alt text for images you post. Alt text is far more prevalent on Mastodon than it was in the posts I saw on Twitter. The guides to Mastodon suggest that you post the first post in a thread as public, then make your replies to that post unlisted. That will let anyone interested see the replies, but not spam your followers\u0026rsquo; feeds with a lot of messages they may not be interested in. Tools I found some really helpful tools for the migration\na.gup.pe Guppe is a bot that brings social groups to the fediverse. You follow @TOPIC@a.gup.pe, then if you mention @TOPIC@a.gup.pe in a toot, the bot will boost (Mastodon\u0026rsquo;s version of Twitter\u0026rsquo;s retweet) your post, so everyone else who follows the topic will see it in their feed. You don\u0026rsquo;t have to do anything special to create a topic, just mention it in a post or follow it.\nDebirdify Debirdify will look at your follows (or followers), and scrape their profiles to find Mastodon-style identifiers in the format @username@server.tld. You can then export those lists as CSV files, which you can import into Mastodon by going into Settings -\u0026gt; Import. Make sure you pick MERGE so that it only merges any new handles instead of replacing your existing list.\ntwitter-archive-parser Transforms a downloaded Twitter archive into markdown format, including expanding all t.co URLs with their original versions.\n","permalink":"https://unixorn.github.io/post/switching-to-mastodon/","summary":"\u003cp\u003eI\u0026rsquo;m moving from Twitter to Mastodon. Specifically, I\u0026rsquo;m @unixorn@hachyderm.io. Here are my first impressions.\u003c/p\u003e","title":"Switching to Mastodon"},{"content":"Some tips about setting up Zigbee or Z-wave mesh networks.\nI\u0026rsquo;ve had a few people new to Home Assistant ask about how to best set up their mesh networks. Here\u0026rsquo;s my take on what works best.\nBefore you start using Zigbee, make sure your Zigbee and 2.4GHz WIFI aren\u0026rsquo;t stepping on each other See https://www.youtube.com/watch?v=t-gw7kURXCk for details, but the TL;DR is that Zigbee uses the same 2.4GHz radio frequency band that 2.4GHz WIFI does, so choose your channels wisely to avoid overlap.\nYou probably already know that wifi channels interfere with each other, so the best channels to use to avoid interference are 1, 6 and 11. Once you put Zigbee into the mix, you need to also know that Wifi channel 1 interferes with zigbee channels 11-17, wifi channel 6 interferes with zigbee channels 13-23, and wifi channel 11 interferes with zigbee channels 18-26. Some zigbee devices don\u0026rsquo;t work with channel 26, so even though it\u0026rsquo;ll get the least interference from wifi, you\u0026rsquo;ll want to rule it out.\nIf you\u0026rsquo;re using multiple wifi access points, you\u0026rsquo;ll want to set them to use different wifi channels so they don\u0026rsquo;t interfere with each other. Your best choices are using wifi channels 1 \u0026amp; 6 with zigbee channel 24, wifi 6 \u0026amp; 11 with zigbee channel 11, or wifi 1 and 11 with zigbee channel 18.\nChanging the Zigbee channel forces you to re-pair every Zigbee device. This sucks, so plan ahead. Changing the 2.4GHz WIFI channel on your access points is a lot less painful. You don\u0026rsquo;t have to worry about this with Z-Wave since it uses 800-900Mhz depending on the country.\nHow to determine what zigbee channel zigbee2mqtt is using Go to Settings, Advanced, and then you\u0026rsquo;ll see your zigbee channel.\nUse a USB extension cable Computers can spew out a lot of radio interference, especially if you\u0026rsquo;re using a single-board computer like a Raspberry Pi or ODROID that has a cheap plastic case that is effectively unshielded. A 15 foot USB extension cable will make a lot of signal issues with your Zigbee or Z-Wave mesh disappear, with the added bonus that you can put the dongle in a location that\u0026rsquo;s not convenient to put a computer. I\u0026rsquo;ve got my servers racked in my basement, but the dongles are on the basement ceiling directly under powered Zwave and Zigbee switches so my meshes are very strong.\nAdd your powered devices first Zigbee and Z-Wave both are mesh networks. If you\u0026rsquo;re doing a new setup, start with your powered devices since they can act as routers. Start with the ones closest to your coordinator, then work your way outward. This way, when you start adding your battery devices the mesh will already have routers in place and the battery devices won\u0026rsquo;t all try to connect directly to the coordinator.\nAdd devices where you\u0026rsquo;re going to use them, not while next to your coordinator In the past, I\u0026rsquo;ve had new Zigbee and Z-Wave devices sometimes show spotty performance. After some experimentation, the problem seemed to be based on where the new device was when I added it to the mesh. When you add a device when it\u0026rsquo;s right next to the mesh coordinator attached to your Home Assistant machine, it will query all the routers it can see, see that the coordinator is available, and to try to minimize hops to the coordinator it will connect directly. This would be no big deal except that when you move it somewhere else (like when I moved a new smart plug from the basement next to my Home Assistant machine to the second floor), it will still try to talk directly to the coordinator rather than any adjacent routers and have sub-optimal performance.\nInstead, take it to wherever you\u0026rsquo;re going to use it and plug it in there and pair it there. This will let it detect which of your routers have the strongest signals and connect to them instead of trying to connect directly to the coordinator. If you\u0026rsquo;re adding a new router like a smart plug, this will immediately strengthen your mesh.\nIt\u0026rsquo;s not a huge deal if you don\u0026rsquo;t do it this way. With Zigbee the coordinator will eventually get around to rebalancing the routing connections. Z-Wave will require you to heal the mesh, it doesn\u0026rsquo;t do it automagically in the background like Zigbee does.\nYou should heal your Z-Wave network every time you add a device. When you first add a device to the Z-Wave nework, only the device it initially connects to will know it exists and be able to route messages to the new device. Healing the network makes all the devices in your mesh learn about all the other devices - until they learn about the other devices, you will end up with some wonky message routing paths.\nTL;DR - Add powered devices to your mesh first. Add new devices to your mesh where you\u0026rsquo;re actually going to use them. If you\u0026rsquo;re using Z-Wave, you should periodically heal your Z-Wave mesh so it can re-calculate optimum routing. Heal the Z-Wave mesh even if you haven\u0026rsquo;t added, moved or removed any devices - sometimes even moving furniture around can affect the signal between devices, so give it a chance to recalculate optimum routing.\nEdit: Added instructions for finding zigbee channel on zigbee2mqtt, included optimal wifi-zigbee channel combinations.\n","permalink":"https://unixorn.github.io/post/zigbee-and-zwave-setup-tip/","summary":"\u003cp\u003eSome tips about setting up Zigbee or Z-wave mesh networks.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve had a few people new to Home Assistant ask about how to best set up their mesh networks. Here\u0026rsquo;s my take on what works best.\u003c/p\u003e","title":"Zigbee and Zwave Setup Tips"},{"content":"I wanted to switch my new Home Assistant (HA) installation to write data to PostgreSQL instead of SQLite for a variety of reasons. Here\u0026rsquo;s how I did it.\nHere\u0026rsquo;s why I decided to switch:\nResilience. If you\u0026rsquo;re running Home Assistant on a Raspberry Pi\u0026rsquo;s SD card, the constant churn of history updates will eventually destroy the card. The more entities you have, the faster HA will grind your SD card to failure. Writing all that to another server that is writing to a real SSD or spinning disk eliminates that problem. Convenience. I can back up the postgres database without having to stop HA. I don\u0026rsquo;t even have to run the backup on the HA server. Speed. Using a real database will speed up history display, especially once you have a large number of entities. Pre-requisites Note - I did this on a fresh server with no history I wanted to preserve. What I\u0026rsquo;m describing here will discard all your old history data. If you do have history you want to preserve, there\u0026rsquo;s a forum post here that explains how to load your history from SQLite and into postgres with pgloader.\nA PostgreSQL server. I recommend that you configure your router to assign your server a static IP address so when you reboot it it doesn\u0026rsquo;t get a different IP from the DHCP pool and force you to update your HA server\u0026rsquo;s configuration.yml.\nSetting up postgres Here\u0026rsquo;s an example docker-compose.yaml file that will start postgres for you. Rather than use a docker volume and then have to save and restore that, this configuration will mount /path/to/postgres/data into the container so you can safely destroy and recreate your docker environment without having to restore your database from a backup.\nversion: \u0026#39;3\u0026#39; services: postgres: container_name: postgres image: postgres:14.5 restart: always network_mode: host ports: - \u0026#34;5234:5234\u0026#34; environment: POSTGRES_USER: postgresadmin POSTGRES_PASSWORD: \u0026lt;redacted\u0026gt; volumes: - /path/to/postgres/data:/var/lib/postgresql/data - /etc/localtime:/etc/localtime:ro The first time you run this image, it\u0026rsquo;ll generate the database files, create a user with admin privileges named POSTGRES_USER with password POSTGRES_PASSWORD. Now that it\u0026rsquo;s running, we should create a user just for Home Assistant.\nI set this up on one of my Odroid HC2s so I could keep the data directory on the hard drive there for ease of backup.\nUpdating Home Assistant to use PostgreSQL Now that the postgres server is running, we\u0026rsquo;re going to use psql (the postgres command line client) to set up a Home Assistant user and database. You can install psql on your machine, but I prefer to use a container.\nRun docker run -it --rm postgres:14.5 bash to get a shell running inside a postgres container. It\u0026rsquo;ll have the tools you need to create an account for your Home Assistant instance.\nHere\u0026rsquo;s the commands you\u0026rsquo;re going to need:\npsql CREATE USER homeassistant WITH PASSWORD 'yourHomeAsssistantPassword'; CREATE DATABASE homeassistant_db WITH OWNER homeassistant ENCODING 'utf8' TEMPLATE template0; Now that the postgres server is set up, you can configure your HA server to use it instead of SQLite.\nHere\u0026rsquo;s a snippet from my configuration.yaml\n# Database recorder: db_url: !secret psql_connector_string db_retry_wait: 10 # Wait 10 seconds before retrying exclude: domains: - automation - updater entity_globs: - sensor.weather_* entities: - sun.sun # Don\u0026#39;t record sun data - sensor.last_boot # Comes from \u0026#39;systemmonitor\u0026#39; sensor platform - sensor.date event_types: - call_service # Don\u0026#39;t record service calls To save space, I\u0026rsquo;ve disabled storing changes to sun.sun, sensor.date and sensor.last_boot, along with weather information and calls to services. Tune yours as you see fit.\nI\u0026rsquo;m using a secret for the db_url, and here\u0026rsquo;s a redacted example from my secrets.yaml file showing the proper format:\npsql_connector_string: \u0026#34;postgresql://DATABSE_USERNAME:DATABASE_PASSWORD@DNSNAME_OR_IP_OF_POSTGRES_SERVER/DATABASE_NAME\u0026#34; So in my case, the redacted string is postgresql://hassuser:hasspassword@postgres.example.com/homeassistant_db\nNow that you\u0026rsquo;ve updated the server configuration, confirm that you don\u0026rsquo;t have any typos and your configuration is valid by going to http://yourHA:8123/developer-tools/yaml and clicking CHECK CONFIGURATION. If it reports it valid, you can safely restart HA and you\u0026rsquo;ll be storing all your history data in Postgres.\nBacking Up Your Database Now that we\u0026rsquo;re writing data to postgres, time to take advantage of not having to shut it down for backups and start backing things up.\nI wrote a simple shell script for backing up your postgres database, ha-postgresql-backup. It has reasonable default settings, which you can override for your environment by setting environment variables.\nCOMPRESSOR - If you set this, also set EXTENSION. Defaults looking for bzip2 and gzip, and will use bzip2 if both are found. DUMP_D - What directory to write the dump file to. If you don\u0026rsquo;t set this, it will use the current directory HASS_D - What postgres database to back up. Defaults to homeassistant_db PG_PASSWORD - The ha user\u0026rsquo;s password PG_SERVER - What server to connect to PG_USERNAME - Used to log into your postgres server. Defaults to homeassistant. You should use the same username \u0026amp; password your Home Assistant is using. POSTGRESQL_IMAGE - What docker image to use. Defaults to postgres:14.5 Example usage: PG_PASSWORD=your_ha_pg_password PG_USER=your_ha_pg_user PG_SERVER=pg.example.com HASS_DB=homeassistant_db DUMP_D=/path/to/directory hass-postgresql-backup\nYou don\u0026rsquo;t need to stop Home Assistant to pull a backup, and you also don\u0026rsquo;t need to run it on the same server you\u0026rsquo;re running the postgres server on.\n","permalink":"https://unixorn.github.io/post/hass-using-postgresql-instead-of-sqlite/","summary":"\u003cp\u003eI wanted to switch my new Home Assistant (HA) installation to write data to \u003ca href=\"https://www.postgresql.org/\"\u003ePostgreSQL\u003c/a\u003e instead of SQLite for a variety of reasons. Here\u0026rsquo;s how I did it.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s why I decided to switch:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResilience\u003c/strong\u003e. If you\u0026rsquo;re running Home Assistant on a Raspberry Pi\u0026rsquo;s SD card, the constant churn of history updates will eventually destroy the card. The more entities you have, the faster HA will grind your SD card to failure. Writing all that to another server that is writing to a real SSD or spinning disk eliminates that problem.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConvenience\u003c/strong\u003e. I can back up the postgres database without having to stop HA. I don\u0026rsquo;t even have to run the backup on the HA server.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSpeed\u003c/strong\u003e. Using a real database will speed up history display, especially once you have a large number of entities.\u003c/li\u003e\n\u003c/ul\u003e","title":"Switch Home Assistant to Use PostgreSQL Instead of SQLite"},{"content":"Securifi Peanut plugs have issues with zigbee2mqtt.\nI have several Securifi Peanut Zigbee switches. Overall, they\u0026rsquo;re nice little smart plugs and make good Zigbee routers to strengthen your Zigbee mesh, but they have one annoying issue - zigbee2mqtt doesn\u0026rsquo;t recognize them perfectly, though there\u0026rsquo;s a simple fix that I\u0026rsquo;m going to document here.\nAdding Peanut Smart Plugs to zigbee2mqtt The Peanut Smart Plug does not provide a modelId in its database entry, so zigbee2mqtt can\u0026rsquo;t identify it to know how to handle it. Fortunately, it\u0026rsquo;s an easy fix, though you\u0026rsquo;ll have to do it every time you add a new Peanut plug.\nPair your new Peanut(s) to your zigbee2mqtt instance Once you\u0026rsquo;ve added all the peanut plugs, stop zigbee2mqtt. We\u0026rsquo;re going to need to do some surgery on its database.db file and that can\u0026rsquo;t be done with the service running. Backup your database.db file. If you mess up the edit, you\u0026rsquo;ll want to be able to revert and try again easily. Edit your database.db file. Add a \u0026quot;modelId\u0026quot;:\u0026quot;PP-WHT-US\u0026quot; to each of your Peanut entries. For example, change \u0026quot;\u0026quot;manufId\u0026quot;:4098, to \u0026quot;manufId\u0026quot;:4098,\u0026quot;modelId\u0026quot;:\u0026quot;PP-WHT-US\u0026quot;, Once you\u0026rsquo;ve finished editing database.db, restart the zigbee2mqtt service. You should now see proper entries with capabilities in zigbee2mqtt and be able to turn the switches on and off, both from zigbee2mqtt and from Home Assistant. Now would be a good time to go to zigbee2mqtt\u0026rsquo;s OTA tab and check if your Peanut plug(s) have any firmware updates.\n","permalink":"https://unixorn.github.io/post/securifi-peanut-gotcha/","summary":"\u003cp\u003eSecurifi Peanut plugs have issues with zigbee2mqtt.\u003c/p\u003e\n\u003cp\u003eI have several Securifi \u003ca href=\"https://smile.amazon.com/gp/product/B00TC9NC82\"\u003ePeanut\u003c/a\u003e Zigbee switches. Overall, they\u0026rsquo;re nice little smart plugs and make good Zigbee routers to strengthen your Zigbee mesh, but they have one annoying issue - \u003ca href=\"https://zigbee2mqtt.io\"\u003ezigbee2mqtt\u003c/a\u003e doesn\u0026rsquo;t recognize them perfectly, though there\u0026rsquo;s a simple fix that I\u0026rsquo;m going to document here.\u003c/p\u003e","title":"Fix Securifi Peanut issue with zigbee2mqtt"},{"content":"I wanted my Home Assistant to be able to send me alerts when Bad Things are detected like water on my basement floor. I\u0026rsquo;m an SRE, and have been using PagerDuty for years, so I decided to set up a personal PagerDuty account and connect it to my Home Assistant.\nWhy PagerDuty and not Twilio SMS? When I was working at Twilio I wrote the blog post on their site about using their API with Home Assistant. The down side of using Twilio to notify is that their API doesn\u0026rsquo;t have a simple way to mark messages as potential duplicates, so if you have an automation that triggers for some potentially flappy state like motion in the yard you might get 20 messages for the same problem, which is not awesome in the middle of the night.\nWith PagerDuty, you can specify a dedupe_key and it will automatically ignore new alerts on that key until you resolve the incident, without you having to add any complicated logic to your automations or notify service.\nPrerequisites A working Home Assistant installation. Initial set up of Home Assistant is out of scope for this post - presumably you\u0026rsquo;re reading this because you already have Home Assistant running and want to enable it to send you notifications A PagerDuty account. They have a free tier that is probably more than adequate for most people\u0026rsquo;s needs. Setup PagerDuty Create your PagerDuty account at PagerDuty.com and create a new service. I unimaginatively named mine \u0026ldquo;Home Assistant\u0026rdquo;. You\u0026rsquo;ll also need to configure your notification settings in your profile. If you don\u0026rsquo;t have their app installed, you can install it now, but PagerDuty can send you SMS messages and you can interact with it that way if you prefer.\nIn the new service, click the Integrations tab. We\u0026rsquo;re going to want to use v2 of the Events API to create alerts, so click on the down arrow to the right of Events API V2 to reveal details. We\u0026rsquo;re going to need both the Integration Key and the Integration URL (Alert Events), so copy them into a scratch file for later.\nSchedules PagerDuty will have created a default schedule for your service, which is great. What\u0026rsquo;s not great is that it defaults to alerting 24 hours a day - I don\u0026rsquo;t want to get woken up because of something minor like a network issue, especially on weekends, so I edited my schedule to only be active between 10 am and 10 pm.\nHome Assistant Now we\u0026rsquo;ll connect PagerDuty to Home Assistant. I didn\u0026rsquo;t want to install a helper script into my HA system, and conveniently enough you can create alerts using PagerDuty\u0026rsquo;s API directly, so we\u0026rsquo;re going to create a rest_command to make the API calls for us.\nTo keep our configuration.yaml easier to share, we want to use !secret, so first, edit your secrets.yaml file and add a pd_integration_key entry with the integration key you copied from your service\u0026rsquo;s Integrations tab.\nAnnoyingly, you can\u0026rsquo;t directly embed a !secret xyz call inside the rest_command\u0026rsquo;s payload, so we\u0026rsquo;re going to have to work around it by creating a sensor (where we can use !secret) and then read that value into the rest_command\u0026rsquo;s payload template. This is ugly but it works.\nAdd a pd_routing_key sensor to your configuration.yaml file\nsensor: - platform: template sensors: pd_routing_key: value_template: !secret pd_integration_key Now that we\u0026rsquo;ve defined our sensor, we can refer to its value inside of the payload template in our rest_command, so define a pagerduty_message command by adding the following yaml snippet to your configuration.yaml. I created the payload in this snippet from the examples in PagerDuty\u0026rsquo;s Incident API documentation.\nIf the URL you copied for Integration URL (Alert Events) is different than the url key in the snippet, change the url value accordingly.\nrest_command: pagerduty_message: url: https://events.pagerduty.com/v2/enqueue method: POST payload: \u0026gt;- { \u0026#34;routing_key\u0026#34;:\u0026#34;{{ states(\u0026#39;sensor.pd_routing_key\u0026#39;) }}\u0026#34;, \u0026#34;dedup_key\u0026#34;:\u0026#34;{{ dedup_key }}\u0026#34;, \u0026#34;event_action\u0026#34;:\u0026#34;trigger\u0026#34;, \u0026#34;payload\u0026#34;: { \u0026#34;summary\u0026#34;:\u0026#34;{{ message }}\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;{{ source }}\u0026#34;, \u0026#34;severity\u0026#34;:\u0026#34;{{ severity }}\u0026#34;, \u0026#34;custom_details\u0026#34;: { \u0026#34;{{custom_title}}\u0026#34;: \u0026#34;{{ custom_details }}\u0026#34; } } } Confirm that you don\u0026rsquo;t have any typos and your configuration is valid by going to http://yourHA:8123/developer-tools/yaml and clicking CHECK CONFIGURATION. If it reports it valid, you can safely restart HA.\nTesting Click on the services tab on your HA\u0026rsquo;s developer tools page. Select RESTful Command: pagerduty_message, and then paste the following snippet in.\nmessage value is the subject of the alert. dedup_key is used to prevent you from getting spammed with multiple alerts for the same problem. Until you mark an incident as resolved, any new alerts with the same dedup_key value will not generate new alerts. I have a different unique dedup_key for each of my water sensor automations so that if water is detected under one of my sinks, I\u0026rsquo;ll only get one alert per sink instead of getting a new alert every five minutes for the same problem. severity- must be critical, error, warning, or info custom_title - This will be displayed in the Details section of alerts. custom_details - This will be in the Details section of alerts. service: rest_command.pagerduty_message data: message: This is the subject line dedup_key: a_unique_string severity: info source: Test Source custom_title: Issue custom_details: Test alert to PagerDuty Click the CALL SERVICE button. You should get an alert almost immediately, depending on your personal notification settings. Once that\u0026rsquo;s working, you can use your new pagerduty_message service in your scripts and automations.\n","permalink":"https://unixorn.github.io/post/use-pagerduty-with-home-assistant/","summary":"\u003cp\u003eI wanted my Home Assistant to be able to send me alerts when Bad Things are detected like water on my basement floor. I\u0026rsquo;m an SRE, and have been using PagerDuty for years, so I decided to set up a personal \u003ca href=\"https://www.pagerduty.com/\"\u003ePagerDuty\u003c/a\u003e account and connect it to my Home Assistant.\u003c/p\u003e","title":"Use Pagerduty With Home Assistant"},{"content":"I signed up for the free personal edition of Autodesk Fusion 360 to get started with 3d printing, and the first time I tried to launch it on macOS, it wouldn\u0026rsquo;t let me in because it refused to create a new team for me to join.\nThe solution ended up being to quit Fusion 360, open the Fusion Team Signup page in a browser, create a team there, then restart Fusion 360.\nI found the solution in a forum post from 2019, so I\u0026rsquo;m not holding my breath waiting for Autodesk to fix it.\n","permalink":"https://unixorn.github.io/post/autodesk-fusion-signup/","summary":"I signed up for the free personal edition of Autodesk Fusion 360 to get started with 3d printing, and the first time I tried to launch it on macOS, it wouldn\u0026rsquo;t let me in because it refused to create a new team for me to join.\nThe solution ended up being to quit Fusion 360, open the Fusion Team Signup page in a browser, create a team there, then restart Fusion 360.","title":"Fixing Autodesk Fusion 360 First Time Launch on macOS Problems"},{"content":"TL;DR - SMR drives can take thirteen to sixteen times as long to resilver in your ZFS raid than CMR drives. If they even succeed. This wouldn\u0026rsquo;t be a big deal, except that Western Digital started using SMR technology in their WD-Red drives that are marketed toward SOHO and small business raid, without any warnings about the RAID performance implications.\nI got lucky when I bought my last batch of Reds, they were all CMR, but it was pure luck - I bought them based on Western Digital\u0026rsquo;s reputation and because I\u0026rsquo;ve seen multiple NAS vendors recommend WD-REDs in the past.\nHere\u0026rsquo;s some articles with more details.\nSurreptitiously Swapping SMR into Hard Drive Lines Must Stop IXSystems (the TrueNAS/FreeNAS vendor)\u0026rsquo;s statement that they will no longer ship SMR drives in their products WD has since forked off a RED PLUS product line with CMR.\nAnyway, check your spares bin for SMR drives, and check the drives in your NAS - if you have SMR drives in the NAS, pull a backup and start swapping in CMR replacements.\n","permalink":"https://unixorn.github.io/post/western-digital-red-smr-fiasco/","summary":"TL;DR - SMR drives can take thirteen to sixteen times as long to resilver in your ZFS raid than CMR drives. If they even succeed. This wouldn\u0026rsquo;t be a big deal, except that Western Digital started using SMR technology in their WD-Red drives that are marketed toward SOHO and small business raid, without any warnings about the RAID performance implications.\nI got lucky when I bought my last batch of Reds, they were all CMR, but it was pure luck - I bought them based on Western Digital\u0026rsquo;s reputation and because I\u0026rsquo;ve seen multiple NAS vendors recommend WD-REDs in the past.","title":"Western Digital Red drive SMR Fiasco"},{"content":"I wrote two articles in sysadvent 2021\nBaking Multi-Architecture Docker Images Setting up k3s in your home lab ","permalink":"https://unixorn.github.io/post/sysadvent-2021-article/","summary":"I wrote two articles in sysadvent 2021\nBaking Multi-Architecture Docker Images Setting up k3s in your home lab ","title":"Sysadvent 2021 Articles"},{"content":"For a variety of reasons, I needed to enable some EC2 instances to write/update a single EC2 tag, but the instaces needed to only be able to tag themselves.\nThis was more annoying than I expected, so I\u0026rsquo;m documenting the IAM policy here.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DescribeInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:ARN\u0026#34;: \u0026#34;${ec2:SourceInstanceARN}\u0026#34; }, \u0026#34;ForAllValues:StringEquals\u0026#34;: { \u0026#34;aws:TagKeys\u0026#34;: \u0026#34;THAT_ONE_ALLOWED_TAG\u0026#34; } } } ] } Some notes:\nThe AWS IAM editor in the webui will (as of June 2021) complain about SourceInstanceARN. Ignore it and click next. Then it will complain that the policy doesn\u0026rsquo;t add any permissions. It lies. Ignore it and save the policy. You can attach this policy to an IAM role and the instances will then be able to tag themselves, but only with the THAT_ONE_ALLOWED_TAG tag.\n","permalink":"https://unixorn.github.io/post/iam-self-tagging/","summary":"\u003cp\u003eFor a variety of reasons, I needed to enable some EC2 instances to write/update a single EC2 tag, but the instaces needed to only be able to tag themselves.\u003c/p\u003e","title":"AWS IAM Self Tagging EC2 Instances"},{"content":"I wanted to set up a security camera outside, but I didn\u0026rsquo;t want to be dependent on an outside cloud service - if my internet goes out, I don\u0026rsquo;t want to lose my ability to record video.\nWyze cameras are nice and cheap, and you can reflash them to support RTSP in addition to streaming to the Wyze cloud.\nSetup Prerequisites A camera that supports the RTSP protocol (I\u0026rsquo;m using a Wyze G2) A spare microSD card for reflashing the Wyze G2 camera An x86 machine running docker. As of 2021-03-14, Shinobi only publishes an amd64 version of the shinobisystems/shinobi docker image. A reasonable amount of disk space - the Wyze G2 I\u0026rsquo;m using generates around 330 megs per hour of stored 1080p video. Camera Setup I wanted a camera that supported the Real Time Streaming Protocol (RTSP) because that is an open standard which works with a wide variety of tooling, both Open Source and commercial.\nI looked at a variety of camera options, and Wirecutter\u0026rsquo;s Best Wifi Home Security Camera listed the Wyze G2 as runner-up. It and the first choice (Eufy 2K Indoor cam) both support RTSP, but the Wyze was in stock (and half the price at $26) so I went with it.\nI did have to reflash the Wyze G2 to enable a beta firmware that supports both Wyze\u0026rsquo;s cloud and RTSP. Conveniently, it can stream to both simultaneously, so I can watch the streams with the Wyze app when away from home and still record everything to my homelab cluster.\nReflash the Wyze Camera Wyze now has a beta firmware that simultanously supports both their cloud offering and RTSP. Note that they\u0026rsquo;ve explicitly stated that the RTSP branch will get features later than the mainline firmware. I personally don\u0026rsquo;t care, but it is something to consider if you\u0026rsquo;re going to want to use bleeding edge features.\nThe official instructions for reflashing the G2 camera are on the Wyze Cam RTSP page and clear, so I\u0026rsquo;m not going to rehash them here. You\u0026rsquo;ll need a FAT32 formatted microSD card to do the firmware reflash.\nAfter you reflash the camera, you\u0026rsquo;ll need to configure a username/password combination for the camera stream using the Wyze phone app.\nBefore you configure the camera, I recommend that you go into your router\u0026rsquo;s configuration and assign the camera a static IP so that your DVR doesn\u0026rsquo;t lose the stream connection when the camera or router are rebooted. You can also hardcode an IP address into the G2 camera\u0026rsquo;s configuration, but I prefer to keep all the static IP assignments for my network in one place, the DHCP configuration on my router.\nYou\u0026rsquo;ll end up with a rtsp url that looks like rtsp://username:password@192.168.1.101/live.\nDVR Setup I don\u0026rsquo;t use any IOT devices that require a cloud service to function. In this case, I especially do not want to be unable to record security footage just because the internet is down, so I set up shinobi as a local DVR to record my security footage.\nStart shinobi I\u0026rsquo;m running shinobi in a docker container. As of 2021-03-14, there is only an AMD64 build of this docker image so I\u0026rsquo;m running it on the Intel machine in my homelab.\nHere\u0026rsquo;s a shinobi-start script:\n#!/usr/bin/env bash # # Start shinobi # # Copyright 2021, Joe Block \u0026lt;jpb@unixorn.net\u0026gt; # # License: Apache 2.0 SHINOBI_D=${SHINOBI_D:-\u0026#39;/data/shinobi\u0026#39;} set -o pipefail if [[ -n \u0026#34;$DEBUG\u0026#34; ]]; then set -x fi for dvr_d in config customAutoLoad database plugins video do mkdir -p \u0026#34;$SHINOBI_D/$dvr_d\u0026#34; done exec docker run -d -p 8080:8080 \\ --name=\u0026#39;shinobi\u0026#39; \\ -v ${SHINOBI_D}/config:/config:rw \\ -v ${SHINOBI_D}/customAutoLoad:/home/Shinobi/libs/customAutoLoad:rw \\ -v ${SHINOBI_D}/database:/var/lib/mysql:rw \\ -v ${SHINOBI_D}/plugins:/plugins:rw \\ -v ${SHINOBI_D}/videos:/home/Shinobi/videos:rw \\ -v /dev/shm/Shinobi/streams:/dev/shm/streams:rw \\ -v /etc/localtime:/etc/localtime:ro \\ -v /etc/timezone:/etc/timezone:ro \\ --restart always \\ shinobisystems/shinobi Run this with SHINOBI_D=/path/to/local/dvr/files shinobi_start and it will create any missing required directories for you and start shinobi.\nConfigure shinobi Set up a new admin account Login at http://your.shinobi.server:8080/super with username admin@shinobi.vido and password admin Create a new admin account Don\u0026rsquo;t forget to reset the password for the admin@shinobi.video account! Add the camera Login at http://your.shinobi.server:8080 Click on the + icon in the toolbar at the top of the page Set mode to record Change the name to something human friendly like \u0026ldquo;Mailbox Camera\u0026rdquo; Set input type (in the connection section) to H.264 / H.265 / H.265+ Set the full URL path to the rtsp stream url you got from the camera Optionally set Skip Ping to Yes Set Stream Type to HLS (includes audio) Set Record File Type to MP4 Set Video codec to copy Set Audio Codec to Auto Save You can optionally set retention times for the camera data.\nIt took about 30-45 seconds before my camera stream was visible in shinobi.\n","permalink":"https://unixorn.github.io/post/setting-up-shinobi-and-a-wyze-g2-camera/","summary":"\u003cp\u003eI wanted to set up a security camera outside, but I didn\u0026rsquo;t want to be dependent on an outside cloud service - if my internet goes out, I don\u0026rsquo;t want to lose my ability to record video.\u003c/p\u003e\n\u003cp\u003eWyze cameras are nice and cheap, and you can reflash them to support RTSP in addition to streaming to the Wyze cloud.\u003c/p\u003e","title":"Setting up Shinobi and a Wyze G2 Camera"},{"content":"I\u0026rsquo;ve got an old HP laser printer in my basement. We barely print 10 pages a month between the two of us, so we only turn it on when we\u0026rsquo;re going to print. That\u0026rsquo;s a hassle though, because inevitably we forget to shut it off sometimes and it stays on overnight or even for days, and while it has a powersave mode, the 4050N is so old that even that burns a good amount of power.\nEnter Home Assistant.\nPrerequisites You have HA configured to connect to a MQTT server The watcher script and associated tooling all presume that we can send messages to a MQTT topic that HA is watching.\nYour printer is connected to a cupsd server running in a container Your computers should be configured to print to the cupsd server instead of directly to the printer.\nI run cupsd in a container on one of my Odroids. I could run it on the same Odroid HC2 that I run Home Assistant (HA) on, but there\u0026rsquo;s no compelling reason to do so and I\u0026rsquo;m reserving that node for strictly HA containers like Home Assistant itself and my MQTT server. I picked an Odroid because it has a SATA drive attached and my /var/lib/docker is on the hard drive and not an microSD card - there\u0026rsquo;s no reason you can\u0026rsquo;t run it on a Raspberry Pi other than to prevent excessive wear on the microSD card.\nYou could modify the watcher script if you\u0026rsquo;re running cupsd directly instead of in a container, but I run my cupsd in a container, so that\u0026rsquo;s what the script is designed for.\nThere are plenty of articles about setting up cupsd, but I wrote about setting up cupsd here.\nYour printer is plugged into an outlet controlled by HA We want to be able to toggle the power from Home Assistant.\nPrinter Power Control mosquitto helper script I don\u0026rsquo;t like to install anything more on my docker hosts than I absolutely have to, so instead of installing mosquitto directly on the printserver machine, I run mosquitto_pub inside a container with the following c-mosquitto_pub helper script. You can download it from github here. Put this in /usr/local/bin.\n#!/usr/bin/env bash # # Use docker to run mosquitto_pub # # Copyright 2019, Joe Block \u0026lt;jpb@unixorn.net\u0026gt; # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. exec docker run -t --rm eclipse-mosquitto mosquitto_pub $@ cupsd Watcher Once I had cupsd configured to share the printer (as Franklin), I wrote a quick script that checks the print queue to see if it is empty or not. If there are jobs in the queue, it writes ON to an MQTT topic, hass/printers/franklin. If the queue is empty, it writes OFF. The examples here all assume your printer is named Franklin, replace Franklin with your printer\u0026rsquo;s name.\nActually, I lied. When there are jobs, it writes OFF and then ON.\nWhy? Because I don\u0026rsquo;t want HA to switch the printer off immediately once the queue drains - the printer has enough RAM that there may still be several pages left to print when it has accepted all of the job from the server.\nInstead, I\u0026rsquo;ve configured HA to restart a timer every time it sees the MQTT topic hass/printers/franklin switch from OFF to ON, and only turn the printer off after the queue has been empty for five continuous minutes.\nHere\u0026rsquo;s the ha-check-for-print-jobs script source - you can download it from github here.\nPut the script in /usr/local/bin on the same server you\u0026rsquo;re running the cupsd container on - it is designed to run a tool inside that container.\n#!/usr/bin/env bash # # ha-check-for-print-jobs # # Check if there are print jobs on $PRINT_Q. If there are, write # MQTT messages to a watched topic so HA knows to turn on the power # to the printer. # # Copyright 2021, Joe Block \u0026lt;jpb@unixorn.net\u0026gt; # # License: Apache 2 set -o pipefail # Make all these overridable easily in your cron setup PRINT_Q=${PRINT_Q:-\u0026#39;Franklin\u0026#39;} CONTAINER=${CONTAINER:-\u0026#39;cupsd-server\u0026#39;} MQTT_HOST=${MQTT_HOST:-\u0026#39;mqtt.example.com\u0026#39;} MQTT_TOPIC=${MQTT_TOPIC:-\u0026#39;hass/printers/franklin\u0026#39;} # We are run out of cron every minute, but I don\u0026#39;t want it to take an # entire minute to turn on the power because I\u0026#39;m impatient and the printer # takes a bit to start up. When we print and walk downstairs, I want it # to have already started printing by the time I get there. If I was # patient, I wouldn\u0026#39;t have bothered to write this tool :-) # # So, when we get run by cron, we check the queue CHECK_COUNT times, with # CHECK_DELAY seconds between each run. CHECK_COUNT=${CHECK_COUNT:-\u0026#39;11\u0026#39;} CHECK_DELAY=${CHECK_DELAY:-\u0026#39;5\u0026#39;} export PATH=\u0026#34;$PATH:/usr/local/bin:/usr/local/sbin\u0026#34; if [[ -f /tmp/printerdebug ]]; then DEBUG=\u0026#39;true\u0026#39; fi # Only spam syslog when DEBUG is set debugout() { if [[ -n \u0026#34;$DEBUG\u0026#34; ]]; then echo \u0026#34;$@\u0026#34; fi } validate-settings(){ debugout \u0026#34;CONTAINER: $CONTAINER\u0026#34; debugout \u0026#34;PRINT_Q: $PRINT_Q\u0026#34; debugout \u0026#34;MQTT_HOST: $MQTT_HOST\u0026#34; debugout \u0026#34;MQTT_TOPIC: $MQTT_TOPIC\u0026#34; valid=\u0026#39;true\u0026#39; if [[ -z \u0026#34;$CONTAINER\u0026#34; ]]; then echo \u0026#34;CONTAINER is unset\u0026#34; valid=\u0026#39;false\u0026#39; fi if [[ -z \u0026#34;$PRINT_Q\u0026#34; ]]; then echo \u0026#34;PRINT_Q is unset\u0026#34; valid=\u0026#39;false\u0026#39; fi if [[ -z \u0026#34;$MQTT_HOST\u0026#34; ]]; then echo \u0026#34;MQTT_HOST is unset\u0026#34; valid=\u0026#39;false\u0026#39; fi if [[ -z \u0026#34;$MQTT_TOPIC\u0026#34; ]]; then echo \u0026#34;MQTT_TOPIC is unset\u0026#34; valid=\u0026#39;false\u0026#39; fi if [[ \u0026#34;$valid\u0026#34; == \u0026#34;false\u0026#34; ]]; then echo \u0026#34;Configure your settings.\u0026#34; exit 1 fi } print-job-checker() { printjobs=$(docker exec -t \u0026#34;$CONTAINER\u0026#34; lpq -P \u0026#34;$PRINT_Q\u0026#34; | grep -c \u0026#39;no entries\u0026#39;) if [[ \u0026#34;$printjobs\u0026#34; == \u0026#39;1\u0026#39; ]]; then debugout \u0026#34;No jobs in print queue, notifying HA\u0026#34; c-mosquitto_pub -h \u0026#34;$MQTT_HOST\u0026#34; -t \u0026#34;$MQTT_TOPIC\u0026#34; -m OFF else echo \u0026#34;jobs found in print queue, notifying HA\u0026#34; docker exec -t \u0026#34;$CONTAINER\u0026#34; lpq -P \u0026#34;$PRINT_Q\u0026#34; # Set the status off, then back to on, so that the HA timer restarts # and HA doesn\u0026#39;t turn off the printer in the middle of a job c-mosquitto_pub -h \u0026#34;$MQTT_HOST\u0026#34; -t \u0026#34;$MQTT_TOPIC\u0026#34; -m OFF c-mosquitto_pub -h \u0026#34;$MQTT_HOST\u0026#34; -t \u0026#34;$MQTT_TOPIC\u0026#34; -m ON debugout \u0026#34;re-enabling printer $PRINT_Q...\u0026#34; docker exec -t \u0026#34;$CONTAINER\u0026#34; lpadmin -p \u0026#34;$PRINT_Q\u0026#34; -o printer-error-policy=retry-current-job fi } validate-settings # We run the print-job-checker every 5 seconds to minimize the UI delay on the # macOs end for i in $(seq $CHECK_COUNT) do print-job-checker debugout \u0026#34;waiting...\u0026#34; sleep $CHECK_DELAY done Home Assistant Setup I configured my HA to watch a MQTT topic as a binary sensor. You can download this snippet here.\nbinary_sensor: - platform: mqtt name: \u0026#34;Franklin Print Queue\u0026#34; payload_on: \u0026#34;ON\u0026#34; state_topic: \u0026#34;hass/printers/franklin\u0026#34; Now, when the watcher writes ON and OFF to the hass/printers/franklin queue, that binary sensor will change status and we can trigger an automation for it.\nThis automation will turn the printer power on every time the binary sensor is turned on, and turn it off five minutes after the last time the binary sensor switched from ON to OFF.\nThe outlet my printer is plugged into is controlled by HA and rather unimaginatively named switch.printerpower.\nAdd this stanza to your automations.yaml file. Download it here.\n# Franklin power is controlled by MQTT - alias: \u0026#39;Turn on Franklin when there are jobs in the queue\u0026#39; trigger: platform: state entity_id: binary_sensor.franklin_print_queue to: \u0026#39;on\u0026#39; action: service: homeassistant.turn_on entity_id: switch.printerpower - alias: \u0026#39;Turn off printer 5 minutes after print queue drains\u0026#39; trigger: platform: state entity_id: binary_sensor.franklin_print_queue to: \u0026#39;off\u0026#39; for: minutes: 5 action: service: homeassistant.turn_off entity_id: switch.printerpower Test the pieces Print server check Confirm that you\u0026rsquo;ve got the print queue configured correctly by running docker exec -it cupsd-server lpq -P Franklin. If there are no jobs, it should print something like\nFranklin is ready no entries Automation test Reload your automations, and you should now be able to test that the automations are correct by running c-mosquitto_pub -h mqtt.yourdomain.com -t hass/printers/franklin -m OFF or -m ON and watch HA turn the power to your printer off and on.\nOnce that is working, print a job, and if you run ha-check-for-print-jobs the printer power should get turned on.\nRun it all automatically Now that you\u0026rsquo;ve confirmed that the power is being cycled properly when the MQTT queue recieves messages and that the print job checker is seeing the printer queue, we can add the checker job to cron.\nAdd\n* * * * * PRINT_Q=Franklin MQTT_HOST=mqtt.example.com MQTT_TOPIC=hass/printers/franklin CONTAINER=cupsd_server /usr/local/bin/ha-check-for-print-jobs | logger -t printserver to your /etc/crontab, and you\u0026rsquo;re good to go. Now every minute, the checker script will get run by cron, and it will check every five seconds for print jobs and exit before the next invocation by cron.\n","permalink":"https://unixorn.github.io/post/home-assistant-printer-power-management/","summary":"\u003cp\u003eI\u0026rsquo;ve got an old HP laser printer in my basement. We barely print 10 pages a month between the two of us, so we only turn it on when we\u0026rsquo;re going to print. That\u0026rsquo;s a hassle though, because inevitably we forget to shut it off sometimes and it stays on overnight or even for days, and while it has a powersave mode, the 4050N is so old that even that burns a good amount of power.\u003c/p\u003e\n\u003cp\u003eEnter Home Assistant.\u003c/p\u003e","title":"Home Assistant Printer Power Management"},{"content":"I have an old HP 4050N. For a variety of reasons, I want to have it behind a print server instead of having my laptops print directly to it. Here\u0026rsquo;s how I set that up.\nPrerequisites A Raspberry Pi (or honestly any linux box) running docker. I like the Raspberry Pi and Odroid HC2 for this sort of thing because they have very low power requirements. A printer supported by the CUPS project. Setup Setup is easy. There are several docker images out there you can use, I made one (the source is on github at unixorn/docker-cupsd) because I wanted one that was multi-architecture - my image has AMD64, ARM7 and ARM64 artchitectures all baked into the same image so you don\u0026rsquo;t have to change the image label based on what system you\u0026rsquo;re running it on. It works fine on (at least) Raspberry Pi, Odroids and Intel servers.\nThe unixorn/cupsd docker image is a bit on the large side because I crammed a lot of printer drivers into it, you may want to look for images that only support single printer families.\nWe\u0026rsquo;re going to store printers.conf in a directory outside the container so that we don\u0026rsquo;t lose our printer configuration every time we upgrade our container.\nI run the cupsd server on an Odroid HC2 because I have /var/lib/docker on the 2TB drive attached to it. I could have put it on one of the Raspberry Pis in the cluster, but didn\u0026rsquo;t want it spooling print jobs and causing excessive wear on the rPi\u0026rsquo;s microSD card.\nMake a directory to store your printer configuration. We\u0026rsquo;ll use /docker/cupsd/etc export CUPSD_DIR='/docker/cupsd/etc' touch $CUPSD_DIR/printers.conf Run the cupsd server with sudo docker run -d --restart unless-stopped \\ -p 631:631 \\ --privileged \\ -v /var/run/dbus:/var/run/dbus \\ -v /dev/bus/usb:/dev/bus/usb \\ -v $CUPSD_DIR/printers.conf:/etc/cups/printers.conf \\ unixorn/cupsd You can now connect to http://SERVER:631 and add printers using the web UI.\nWhen adding the printers to your Mac, select Internet Printing Protocol and put in the IP or DNS name of your print server machine.\nThe queues are printers/printername, not printername.\n","permalink":"https://unixorn.github.io/post/cupsd-setup/","summary":"\u003cp\u003eI have an old HP 4050N. For a variety of reasons, I want to have it behind a print server instead of having my laptops print directly to it. Here\u0026rsquo;s how I set that up.\u003c/p\u003e","title":"Run a CUPSD print server on Raspberry Pi"},{"content":"Sending notifications from Home Assistant via Twilio SMS I got a post published at work about How to Receive Alerts from Home Assistant with Twilio SMS, so I won\u0026rsquo;t replicate it here.\nTL;DR - It is super easy to send SMS messages via Twilio SMS with curl.\nUpdate: I wrote another article, Use PagerDuty with Home Assistant this time for using PagerDuty which allows automatically de-duping notifications.\n","permalink":"https://unixorn.github.io/post/home-assistant-notifications-via-twilio/","summary":"Sending notifications from Home Assistant via Twilio SMS I got a post published at work about How to Receive Alerts from Home Assistant with Twilio SMS, so I won\u0026rsquo;t replicate it here.\nTL;DR - It is super easy to send SMS messages via Twilio SMS with curl.\nUpdate: I wrote another article, Use PagerDuty with Home Assistant this time for using PagerDuty which allows automatically de-duping notifications.","title":"Home Assistant Notifications via Twilio SMS"},{"content":"Biden won! Awesome! The fight isn\u0026rsquo;t over though.\nTime to donate to the Georgia Runoff Senate Elections. I\u0026rsquo;ve set up identical fundraisers, this time for all the different Star Trek Series.\nDonate to your favorite series link and we\u0026rsquo;ll see which series has the most fan support.\nTOS - https://secure.actblue.com/donate/ga-tos TNG - https://secure.actblue.com/donate/ga-tng DS9 - https://secure.actblue.com/donate/ga-ds9 Enterprise - https://secure.actblue.com/donate/ga-enterprise Voyager - https://secure.actblue.com/donate/ga-voyager Discovery - https://secure.actblue.com/donate/ga-discovery Lower Decks - https://secure.actblue.com/donate/ga-lowerdecks Picard - https://secure.actblue.com/donate/ga-picard ","permalink":"https://unixorn.github.io/post/2021-runoff-election/","summary":"Biden won! Awesome! The fight isn\u0026rsquo;t over though.\nTime to donate to the Georgia Runoff Senate Elections. I\u0026rsquo;ve set up identical fundraisers, this time for all the different Star Trek Series.\nDonate to your favorite series link and we\u0026rsquo;ll see which series has the most fan support.\nTOS - https://secure.actblue.com/donate/ga-tos TNG - https://secure.actblue.com/donate/ga-tng DS9 - https://secure.actblue.com/donate/ga-ds9 Enterprise - https://secure.actblue.com/donate/ga-enterprise Voyager - https://secure.actblue.com/donate/ga-voyager Discovery - https://secure.actblue.com/donate/ga-discovery Lower Decks - https://secure.actblue.com/donate/ga-lowerdecks Picard - https://secure.","title":"2021 Georgia Runoff Elections"},{"content":"Adding the Plex integration to Home Assistant is pretty straightforward with the exception of finding your Plex token.\nFind Your Plex token: Sign into your Plex.tv account Browse to one of the media files (TV episode, movie) on your connected server. You\u0026rsquo;ll need a Plex pass for this. Select Get info Click View XML Ignore the XML. Look in the URL to the XML page - you\u0026rsquo;ll see Plex-Token\\=XYZZY. That\u0026rsquo;s the token you\u0026rsquo;ll need. Set up the Plex Home Assistant Integration Once you have the token,\nSign into your Home Assistant Go to Configuration -\u0026gt; Integrations Pick Add, and select Plex. Pick the manual setup option Enter your Plex server\u0026rsquo;s address and the token. ","permalink":"https://unixorn.github.io/post/add-plex-to-hass/","summary":"Adding the Plex integration to Home Assistant is pretty straightforward with the exception of finding your Plex token.\nFind Your Plex token: Sign into your Plex.tv account Browse to one of the media files (TV episode, movie) on your connected server. You\u0026rsquo;ll need a Plex pass for this. Select Get info Click View XML Ignore the XML. Look in the URL to the XML page - you\u0026rsquo;ll see Plex-Token\\=XYZZY. That\u0026rsquo;s the token you\u0026rsquo;ll need.","title":"Add Plex to Home Assistant"},{"content":"Before anyone complains about me getting political on what was mainly a tch site, remember that not being political is a luxury that only people not getting screwed by the current political landscape can afford.\nIn no particular order, the attempts to roll back rights for LGBTQ+ citizens, the assaults on womens\u0026rsquo; rights, the handling of the COVID-19 crisis, packing the Supreme Court with a hypocritical last second Justice, putting kids in cages, the ongoing destruction of the political norms that have existed for decades, the ignoring scientific facts, etc, etc all have focused my efforts on supporting one party.\nSince the tech types reading this blog tend to also be Science Fiction/Gaming/Comic book nerds too, I\u0026rsquo;ve set up a contest to see which fandom is most pro-Democrat.\nLinks below (with identical recipient lists) for several geeky fandoms. Donate, and periodically I\u0026rsquo;ll post totals here.\nStar Wars: https://secure.actblue.com/donate/defeatpalpatine Star Trek https://secure.actblue.com/donate/defeattheborg Dr. Who https://secure.actblue.com/donate/defeatthedaleks Supernatural - https://secure.actblue.com/donate/defeatthechuck Marvel - https://secure.actblue.com/donate/defeatthanos DC - https://secure.actblue.com/donate/defeatdarkseid Feel free to share this, and if you want another fandom added, reply to the tweet I originally announced the contest - https://twitter.com/curiousbiped/status/1315650507806060544\nUpdate:\nFinal standings are:\nStar Trek (by a lot) Star Wars Marvel Battlestar Galactica (widening their lead over #3) Supernatural Tie between DC, and Dr. Who. ","permalink":"https://unixorn.github.io/post/2020-elections/","summary":"Before anyone complains about me getting political on what was mainly a tch site, remember that not being political is a luxury that only people not getting screwed by the current political landscape can afford.\nIn no particular order, the attempts to roll back rights for LGBTQ+ citizens, the assaults on womens\u0026rsquo; rights, the handling of the COVID-19 crisis, packing the Supreme Court with a hypocritical last second Justice, putting kids in cages, the ongoing destruction of the political norms that have existed for decades, the ignoring scientific facts, etc, etc all have focused my efforts on supporting one party.","title":"2020 Politics"},{"content":"I\u0026rsquo;ve got a mix of architectures in my basement cluster - some Odroid HC2s that are arm7, some Raspberry Pi 4s that are arm64, and am soon going to add an Intel node as well. It\u0026rsquo;s more hassle than it\u0026rsquo;s worth to have to specify different images for the different architectures. I already build my own copies of images, so I decided to start building all my images as multiarchitecture images.\nThis turned out to be a lot easier than I was expecting - recent stable builds (2.0.4.0 (33772) or higher) of Docker Desktop can build for other architectures by running virtual machines in QEMU, so I can do the whole build on my MacBook Pro instead of baking each architecture separately and stitching them together with a manifest file.\nInstall the latest Docker Desktop for macOS Enable experimental mode either by setting DOCKER_CLI_EXPERIMENTAL=enabled in your environment or by adding \u0026quot;experimental\u0026quot; : \u0026quot;enabled\u0026quot; to ~/.docker/config.json Do docker buildx ls to see the current builders docker buildx create --name multiarch docker buildx use multiarch docker buildx inspect --bootstrap Now you\u0026rsquo;re ready to build. Go into one of your docker projects, then do docker buildx --platform linux/amd64,linux/arm64,linux/arm/v7 -t username/demo --push .\nIf you\u0026rsquo;re not ready to push your image to docker hub, do --load instead of --push to have it build the image and copy it out of the buildx system and into your local docker.\nEdit - documented enabling Docker\u0026rsquo;s experimental mode ","permalink":"https://unixorn.github.io/post/multi-architecture-images/","summary":"\u003cp\u003eI\u0026rsquo;ve got a mix of architectures in my basement cluster - some Odroid HC2s that are \u003ccode\u003earm7\u003c/code\u003e, some Raspberry Pi 4s that are \u003ccode\u003earm64\u003c/code\u003e, and am soon going to add an Intel node as well. It\u0026rsquo;s more hassle than it\u0026rsquo;s worth to have to specify different images for the different architectures. I already build my own copies of images, so I decided to start building all my images as multiarchitecture images.\u003c/p\u003e","title":"Building Multi Architecture Docker Images with buildx"},{"content":"I wanted a machine with more memory to be the master node for my ARM k3s cluster. I had an Odroid N2 with 4GB of RAM, sitting around, so here\u0026rsquo;s the log of getting it installed and running.\nParts ODROID N2 MMC card with preloaded Ubuntu Installation Base Login as root. Default password is odroid\nChange the root password! passwd Disable XWindow - systemctl set-default multi-user.target Purge XWindow packages - apt purge 'x11-*' \u0026amp;\u0026amp; apt autoremove Now that we\u0026rsquo;ve gotten rid of XWindow, get up to date - apt update \u0026amp;\u0026amp; apt upgrade Install some useful tools apt install -y zsh git htop iotop vim libssl-dev software-properties-common python3-pip Disable Ubuntu motd spam sed -i 's/^ENABLED=.*/ENABLED=0/' /etc/default/motd-news Fix locale complaints during login locale-gen en_US.UTF-8 Docker Add docker repository key - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the repo:\nsudo add-apt-repository \\ \u0026#34;deb [arch=arm64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026#34; Install docker community edition apt update \u0026amp;\u0026amp; apt install docker-ce docker-ce-cli containerd.io\n#!/usr/bin/env bash # # Put the setup all in one script. Run as root with sudo. # Disable XWindow login systemctl set-default multi-user.target # This is going to sit on a shelf in my rack, so purge xwindow apt remove xorg apt purge \u0026#39;x11-*\u0026#39; apt autoremove # Add docker apt repository add-apt-repository \\ \u0026#34;deb [arch=arm64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026#34; # Now that we\u0026#39;ve purged xwindow, upgrade what\u0026#39;s left apt-get update \u0026amp;\u0026amp; apt-get upgrade apt-get install -y \\ git \\ htop \\ iotop \\ libssl-dev \\ python3-pip \\ vim vim-doc vim-scripts \\ zsh # Install docker community edition apt update \u0026amp;\u0026amp; apt install docker-ce docker-ce-cli containerd.io # Finally do some cleanups # Fix locale whining locale-gen en_US.UTF-8 # Disable Ubuntu motd spam sed -i \u0026#39;s/^ENABLED=.*/ENABLED=0/\u0026#39; /etc/default/motd-news ","permalink":"https://unixorn.github.io/post/setting-up-an-n2/","summary":"\u003cp\u003eI wanted a machine with more memory to be the master node for my ARM k3s cluster. I had an Odroid N2 with 4GB of RAM, sitting around, so here\u0026rsquo;s the log of getting it installed and running.\u003c/p\u003e","title":"Setting up an ODROID N2"},{"content":"Motivation I wanted to ensure any data I put into my ARM k3s cluster is backed up to prevent data loss.\nI no longer recommend duplicacy. Instead, read my article on restic backups on TrueNas instead.\nBackup Contenders I took a look at CrashPlan, restic and Duplicacy.\nCrashPlan - Even though they have a decent linux client, I eliminated Crashplan because:\nThey\u0026rsquo;ve already abandoned the home market. I currently use their CrashPlan for Small Business account for my Mac. I suspect they\u0026rsquo;ll also abandon this market because accounts with a small number of licenses also aren\u0026rsquo;t worth their time. They bill per-machine instead of by the amount of storage used. restic - Open source, which is great, but I ended up eliminating them because their deduplication wasn\u0026rsquo;t as strong as Duplicacy. It also seemed a little awkward to prune snapshots when I experimented with it.\nDuplicacy - I chose Duplicacy because it:\nSupports cross-source deduplication Works well with B2 Runs on Linux, Windows and Mac Allows multiple source directories to be backed up simultaneously to the same B2 bucket. Continues backing up where it leaves off after being interrupted and restarted. This eliminates having to completely restart the backup and re-upload everything. It didn\u0026rsquo;t hurt that I know several people using it with large amounts of data who are happy with it.\nOn to the Backups I made a docker image, thoth-duplicacy, which installs duplicacy and duplicacy-util on top of debian buster-slim, along with some helper scripts to make using it more convenient.\nThe image is published on docker hub, with both an Intel and and ARM7 version - the most current builds are tagged unixorn/thoth-duplicacy:armv7l and unixorn/thoth-duplicacy:x86_64.\nUsage For simplicity, I\u0026rsquo;m running my backups as kubernetes cron jobs. This allows me to easily run backups of multiple directory trees at once, and the kubernetes scheduler will automagically spread them around the cluster to the least loaded nodes.\nPre-requisites Create a Kubernetes Namespace I like my cluster neat and organized, with different services in their own namespaces, so I created a backups namespace by running kubectl create namespace backup.\nSet up a B2 Storage Bucket for Duplicacy Create a B2 Bucket and App Key Create a bucket in B2. Only use this bucket for duplicacy backups.. Do this first so that when you create the app key, using the dropdown menu you can easily restrict its access to only this bucket. Create an app key in B2 that you only use with Duplicacy. Definitely do not use the root account\u0026rsquo;s credentials. When you create it, specify that it\u0026rsquo;s only allowed to use your backups bucket. Make sure to copy the app key information when you create it, it will only be displayed once. Now you\u0026rsquo;re ready to initialize the bucket for the first directory you want to back up. The easiest way to do this is by running duplicacy inside the thoth-duplicacy container with docker-compose.\nSet up thoth-duplicacy container git clone git@github.com:unixorn/thoth-duplicacy.git BACKUP_LOCATION=/that/first/directory docker-compose run thoth-duplicacy bash cd /data mkdir -p .duplicacy Initialize the B2 Bucket. duplicacy init -encrypt -storage-name b2 STORAGEID b2://yourbucket. STORAGEID cannot have spaces or any special characters besides - and _. duplicacy will prompt you for the B2 app ID, app key, and the encryption password for your backups. Store the password in your secure password manager - without it, you can\u0026rsquo;t restore any of your data. Annoyingly you have to also set the password, B2 id and key again after initializing the bucket so that backups won\u0026rsquo;t prompt you for them. Set the B2 ID - duplicacy set -storage b2://net-unixorn-blog-test -key b2_id -value YOUR_APP_ID Set the B2 key - duplicacy set -storage b2://net-unixorn-blog-test -key b2_key -value YOUR_APP_KEY Set the password - duplicacy set -storage b2://net-unixorn-blog-test -key password -value YOURPASSWORD You can now run backup-cronjob and watch the first backup grind.\nAfter I configured duplicacy for the first time, it was much less hassle to copy the .duplicacy/preferences json file to each new directory tree. I wanted to back up the .duplicacy directory and change the id key to a new unique one — don\u0026rsquo;t put spaces or any special characters in the id other than _ and -. You don\u0026rsquo;t have to change the storage key, and actually shouldn\u0026rsquo;t - sharing the same storage bucket is what allows duplicacy to deduplicate your files across multiple source directories, which keeps your storage bill down.\nHere\u0026rsquo;s an example preferences file -\n[ { \u0026#34;name\u0026#34;: \u0026#34;b2\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;UNIQUE_ID_FOR_YOUR_DIRECTORY\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;storage\u0026#34;: \u0026#34;b2://your-backups-bucket\u0026#34;, \u0026#34;encrypted\u0026#34;: true, \u0026#34;no_backup\u0026#34;: false, \u0026#34;no_restore\u0026#34;: false, \u0026#34;no_save_password\u0026#34;: false, \u0026#34;nobackup_file\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;keys\u0026#34;: { \u0026#34;b2_id\u0026#34;: \u0026#34;ROLE_ACCOUNT_B2_ID\u0026#34;, \u0026#34;b2_key\u0026#34;: \u0026#34;ROLE_ACCOUNT_B2_KEY\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SUPER_SECRET_ENCRYPTION_PASSWORD_FOR_YOUR_DATA\u0026#34; } } ] Backing up a Directory Tree Here\u0026rsquo;s a sample job that backs up one of my directory trees. I\u0026rsquo;m using the backups namespace that I created earlier to keep things tidy - if you want to use the default namespace instead, delete the namespace entry in the metadata section.\nHere\u0026rsquo;s some things you\u0026rsquo;ll need to customize if you base a job on this example:\nChange the namespace entry in the metadata section to match whichever namespace you decided to use. I run this on Odroid HC2s and Raspberry Pis, which both use ARM CPUs. If you\u0026rsquo;re using x86, change the image entry to unixorn/thoth-duplicacy:x86_64 in the template spec section. I work from home, so I want to restrict the number of upload threads so that running backups don\u0026rsquo;t burn all my upload bandwidth. Change DUPLICACY_BACKUP_THEAD_COUNT in the env section if you want more simultaneous threads. The odroids only have 8 cores, but I had no issues running 12 threads other than gobbling up upstream bandwith. The B2_STORAGE_NAME environment variable is used by the backup-cronjob script to determine where to write the backup, so alter the value according to your setup. I\u0026rsquo;m backing up a moosefs distributed file system. I had already tagged all my chunk servers with kubectl label node NODENAME odroid=true and I use a nodeSelector stanza in the backup cron jobs to restrict the backup to only run on one of the chunk servers where the data resides. The moosefs data is distributed across all the chunk servers and each chunk server in the cluster currently contains 33% of the files, so running the backup on a chunk server maximizes the amount of data that can be local reads and don\u0026rsquo;t have to go across the network. Update or delete the nodeSelector clause to work with your environment. Once you\u0026rsquo;ve updated the file, install the cronjob with kubectl apply -f backup-example-directory-tree.yml.\nYou can download this from backup-example-directory-tree.yml instead of hassling with copy and paste.\napiVersion: batch/v1beta1 kind: CronJob metadata: name: backup-exampledir namespace: backups spec: schedule: \u0026#34;35 */2 * * *\u0026#34; jobTemplate: spec: # Ensure only one copy of the backup is running, even if it takes # so long to run that it is still running when the next cron slot # occurs concurrencyPolicy: Forbid template: spec: containers: - name: backup-exampledir # I\u0026#39;m running this on the odroids in my cluster, so I\u0026#39;m specifying # the ARM7 build image: unixorn/thoth-duplicacy:arm7l # Use the x86_64 tag if you\u0026#39;re on Intel # image: unixorn/thoth-duplicacy:x86_64 args: - /bin/sh - -c - /usr/local/bin/backup-cronjob volumeMounts: - name: data-volume mountPath: /data/ env: # I want to restrict the number of threads used for uploads # so that duplicacy doesn\u0026#39;t consume all my upload bandwidth. # I don\u0026#39;t care if it makes my backups slower. - name: DUPLICACY_BACKUP_THEAD_COUNT value: \u0026#34;3\u0026#34; # backup-cronjob needs to know what defined storage to back up # files to. - name: B2_STORAGE_NAME value: \u0026#34;b2\u0026#34; restartPolicy: OnFailure # Keep it running on a chunkserver so that at least part of the # I/O is to local disk instead of across the network. Remove if # you don\u0026#39;t care what node backups happen on. nodeSelector: odroid: \u0026#34;true\u0026#34; volumes: - name: data-volume hostPath: # This will be remapped to /data which is where duplicacy # expects to find the data it is backing up, and the .duplicacy # directory with its settings. path: /dfs/volumes/exampledir # this field is optional type: Directory Pruning Snapshots I don\u0026rsquo;t want to keep snapshots forever, so I made a kubernetes cron job to clean them up.\nBriefly, you can specify multiple -keep X:Y arguments, where you keep one snapshot for every X days after the snapshots are older than Y days.\nFor example, in the purge-stale-duplicacy-snapshots.yml job below, I have it set with -keep 0:365 -keep 30:90 -keep 7:30 -keep 1:2, which means keep no snapshots more than 365 days old, for snapshots older than 90 days keep one every 30 days, after fourteen days keep one every seven days, and after two days keep one every day.\nWarning: Notice that I specified the expiration rules starting with the longest (365 days) and continuing in descending age order - a minor annoyance with duplicacy is that you have to specify the -keep clauses starting with the longest age threshold and then specify the rules for shorter thresholds, or duplicacy will ignore the rules specified out of order, which could lead to more snapshots being purged than you would expect. Run with -dry-run first so you can see whether all your rules are being applied as you expect.\nBefore using this job definition, at a minimum you should set the namespace for the cron job, update the image if you\u0026rsquo;re running on x86, and update the -keep X:Y statements to correspond with your snapshot retention policy.\nOnce you\u0026rsquo;ve updated the configuration, install the cron job with kubectl apply -f purge-stale-duplicacy-snapshots.yml\nYou can download this from purge-stale-duplicacy-snapshots.yml instead of hassling with copy and paste.\napiVersion: batch/v1beta1 kind: CronJob metadata: name: purge-stale-duplicacy-snapshots namespace: backups spec: schedule: \u0026#34;48 */3 * * *\u0026#34; jobTemplate: spec: concurrencyPolicy: Forbid template: spec: containers: - name: purge-stale-duplicacy-snapshots # I\u0026#39;m running this on the odroids in my cluster, so I\u0026#39;m specifying # the ARM7 build image: unixorn/thoth-duplicacy:arm7l # Use the x86_64 tag if you\u0026#39;re on Intel # image: unixorn/thoth-duplicacy:x86_64 # Make sure we run inside /data so that duplicacy can find # the configuration directory. workingDir: /data # Remember that the -keep arguments must be listed from longest # time frame to shortest, otherwise the disordered ones will be # ignored, which could mean deleting snapshots you want to keep. # # I\u0026#39;m specifying to keep no snapshots more than 365 days old, keep # a single snapshot every 30 days for snapshots older than 90 days, # a single snapshot a week for snapshots older than 30 days, and # finally keep only a single snapshot per day for snapshots # older than 2 days. # # Also note that the duplicacy verb (prune) has to come before # any of the settings command line options. args: - duplicacy - prune - -storage - b2 - -all - -keep 0:365 - -keep 30:90 - -keep 7:14 - -keep 1:2 - -exhaustive volumeMounts: - name: data-volume mountPath: /data/ env: - name: DUPLICACY_BACKUP_THEAD_COUNT value: \u0026#34;3\u0026#34; - name: B2_STORAGE_NAME value: \u0026#34;b2\u0026#34; restartPolicy: OnFailure volumes: - name: data-volume hostPath: # This will be remapped to /data which is where duplicacy # expects to find the data it is backing up, and the .duplicacy # directory with its settings. path: /dfs/volumes/exampledir # this field is optional type: Directory Restoring Files Backups are useless if you can\u0026rsquo;t restore.\nTo restore, use docker-compose and the thoth-duplicacy repository. I only did my test restores with the command line, I haven\u0026rsquo;t bothered experimenting with the GUI from https://duplicacy.com.\nUse git clone git@github.com:unixorn/thoth-duplicacy.git if you didn\u0026rsquo;t keep the checkout when you initialized your B2 bucket Make a directory to restore to, and a .duplicacy subdirectory for the configuration with mkdir -p /path/to/restore/.duplicacy. While you can restore in place over the live directory, I\u0026rsquo;m a bit too cautious to do that especially if I\u0026rsquo;m doing a restore after having already lost files. Copy the preferences file from the directory tree you want to restore to /path/to/restore/.duplicacy. Start a container with BACKUP_LOCATION=/path/to/restore docker-compose run thoth-duplicacy bash Now that you\u0026rsquo;re in a running thoth-duplicacy container with your restore directory mounted as /data, you can restore files. cd /data before running duplicacy commands so it can find its configuration.\nYou can look at the available snapshots with duplicacy list. It will list snapshot name, revision number, and the timestamp when each snapshot was created.\nOnce you know what snapshots are in the bucket, you can examine the files available in a specific snapshot with duplicacy list -files -r REVISION_NUMBER.\nNow you can restore files - if you want to restore just the foo directory, from revision 99, you\u0026rsquo;d run duplicacy restore -r 99 'foo/*'.\n","permalink":"https://unixorn.github.io/post/backing-up-the-cluster-with-duplicacy/","summary":"\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI wanted to ensure any data I put into my \u003ca href=\"https://unixorn.github.io/post/k3s-on-arm/\"\u003eARM k3s cluster\u003c/a\u003e is backed up to prevent data loss.\u003c/p\u003e\n\u003cp\u003eI no longer recommend duplicacy. Instead, read my article on \u003ca href=\"https://unixorn.github.io/post/restic-backups-on-truenas/\"\u003erestic backups on TrueNas\u003c/a\u003e instead.\u003c/p\u003e","title":"Backing Up the Cluster with Duplicacy"},{"content":"Yesterday I had to grow a live filesystem on a server in EC2, without downtime. I do this just infrequently enough to not quite remember all the details without poking around the internet, so I\u0026rsquo;m documenting it all in one place.\nGrow the volume Log into the EC2 console and find your instance. In the description tab, look at the block devices (bottom right as of August 2019) and find the volume you need to grow and get its volume ID. Find that volume in the EBS volumes list. Now is a good time to name it something useful like \u0026ldquo;InstanceName /data01\u0026rdquo; if you haven\u0026rsquo;t already named it. Click Modify Volume, then give it a new size. It may take a minute or two to finish growing the volume, you\u0026rsquo;ll see a percentage displayed. Resize the filesystem Log into the instance and start a tmux or screen session to do all the work in. Getting disconnected in the middle of resizing the filesystem would be bad. Use lsblk to confirm that the EBS block device has increased to the size you expect. If you have partitioned your drive, do sudo growpart /dev/xyz1 0 to grow the partition. Check /etc/fstab to see what format the filesystem is. If you\u0026rsquo;re using xfs, sudo xfs_growfs /dev/DEVICE. If you\u0026rsquo;re using ext2, ext3 or ext4, do sudo resize2fs /dev/DEVICE. If you\u0026rsquo;re using ext2 or ext3, seriously consider replacing this filesystem with an ext4 one during your next downtime window. Wait. Depending on how much larger the EBS volume has become and the instance type, it can take several minutes for the filesystem to finish growing. Confirm the new size with df ","permalink":"https://unixorn.github.io/post/growing-ebs-volumes-in-place/","summary":"\u003cp\u003eYesterday I had to grow a live filesystem on a server in EC2, without downtime. I do this just infrequently enough to not \u003cem\u003equite\u003c/em\u003e remember all the details without poking around the internet, so I\u0026rsquo;m documenting it all in one place.\u003c/p\u003e","title":"Growing EBS Volumes in Place"},{"content":"Why k3s and not stick with k8s? I wanted to experiment with k3s. They package everything you need in a single binary, don\u0026rsquo;t package in deprecated parts of k8s, and it works on Intel, ARMv7 and ARM64. It seemed like it\u0026rsquo;d be a less painful way to runn Kubernetes on my ARM cluster.\nPrerequisites You must have set up DNS entries for the nodes you want to cluster, or update /etc/hosts on all the nodes so they can find each other.\nInstalling k3s I chose to install k3s without the built-in traefik install so I could install that with a custom configuration. I also chose to use docker instead of the baked-in containerd so that I could also run containers outside k3s on my worker nodes without wasting RAM.\nInstalling the master curl -sfL https://get.k3s.io \u0026gt; install-k3s.sh \u0026amp;\u0026amp; chmod 755 ./install-k3s.sh sudo ./install-k3s.sh --no-deploy traefik --docker sudo chgrp docker /etc/rancher/k3s/k3s.yaml sudo chmod g+r /etc/rancher/k3s/k3s.yaml I also updated /etc/systemd/system/k3s.service to add\nAfter=network-online.target cluster-mfsmount.service docker.service\nbecause I don\u0026rsquo;t want k3s to attempt to start until after the docker service has started and the cluster\u0026rsquo;s moosefs distributed filesystem is mounted.\nOnce all that is done, copy the node token from /var/lib/rancher/k3s/server/node-token to each of the worker nodes.\nInstalling the workers Copy /var/lib/rancher/k3s/server/node-token from the server to your worker.\nRun\n./install-k3s.sh --agent --server https://master-server:6443 --kubelet-arg=\u0026#34;address=0.0.0.0\u0026#34; --token \u0026#34;$(cat node-token)\u0026#34; --docker Remove the --docker if you want to use the containerd bundled into k3s - I wanted to be able to also run apps in docker on my nodes and didn\u0026rsquo;t want it using extra RAM for another containerd.\nIf you\u0026rsquo;re using a distributed filesystem like I am, add\nAfter=network-online.target cluster-mfsmount.service docker.service\nto /etc/systemd/system/k3s-agent.service, and\nAfter=network-online.target cluster-mfsmount.service\nto /lib/systemd/system/docker.service to keep docker from starting until after the distributed filesystem is mounted.\nSet up Networking MetallB I wanted to be able to use LoadBalancerIP entries in my cluster services to make using Traefik easier.\nInstalling MetallB On my master node, I ran\nkubectl apply -f https://raw.githubusercontent.com/danderson/metallb/master/manifests/metallb.yaml Configuring MetallB I used the following configuration for metallb (in metallb-conf.yaml)\napiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 10.0.1.16/28 And applied it with kubectl apply -f metallb-conf.yaml.\nThis allows me to use 10.0.1.17 through 10.0.1.30 as LoadBalancerIP entries in my k8s service configurations. 14 entries should be more than enough for my immediate needs.\nYou will want to change the addresses entry to conform to your own network.\nTraefik Installed traefik with my own configuration, which I have posted on github:\nHere are the configuration files I used - you\u0026rsquo;ll need to tweak them for your own network.\ntraefik-rbac.yaml --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system traefik-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: traefik-conf namespace: kube-system data: traefik.toml: | defaultEntryPoints = [\u0026#34;http\u0026#34;,\u0026#34;https\u0026#34;] debug = false logLevel = \u0026#34;INFO\u0026#34; # Do not verify backend certificates (use https backends) InsecureSkipVerify = true [entryPoints] [entryPoints.http] address = \u0026#34;:80\u0026#34; compress = true [entryPoints.https] address = \u0026#34;:443\u0026#34; [entryPoints.https.tls] #Config to redirect http to https #[entryPoints] # [entryPoints.http] # address = \u0026#34;:80\u0026#34; # compress = true # [entryPoints.http.redirect] # entryPoint = \u0026#34;https\u0026#34; # [entryPoints.https] # address = \u0026#34;:443\u0026#34; # [entryPoints.https.tls] [web] address = \u0026#34;:8080\u0026#34; [kubernetes] [metrics] [metrics.prometheus] buckets=[0.1,0.3,1.2,5.0] entryPoint = \u0026#34;traefik\u0026#34; [ping] entryPoint = \u0026#34;http\u0026#34; traefik-deployment.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: kube-system --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lb spec: replicas: 2 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 containers: - image: traefik:1.7.9 name: traefik-ingress-lb volumeMounts: - mountPath: /config name: config ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: admin containerPort: 8080 args: - --api - --kubernetes - --configfile=/config/traefik.toml livenessProbe: httpGet: path: /ping port: 80 initialDelaySeconds: 3 periodSeconds: 3 timeoutSeconds: 1 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: role operator: In values: - data topologyKey: kubernetes.io/hostname volumes: - name: config configMap: name: traefik-conf traefik-service.yaml --- kind: Service apiVersion: v1 metadata: name: traefik-ingress-service namespace: kube-system labels: k8s-app: traefik-ingress-lb spec: selector: k8s-app: traefik-ingress-lb externalTrafficPolicy: Local ports: - protocol: TCP port: 80 name: web - protocol: TCP port: 443 name: https - protocol: TCP port: 8080 name: admin type: LoadBalancer loadBalancerIP: 10.0.1.20 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-ingress-lb namespace: kube-system spec: rules: - host: traefik.example.com http: paths: - path: / backend: serviceName: traefik-ingress-service servicePort: admin You\u0026rsquo;ll want to change the loadBalancerIP entry and the host entry in the spec section to match your network and DNS configurations.\nfor traefik_yaml in traefik-rbac.yaml traefik-configmap.yaml traefik-deployment.yaml traefik-service.yaml do kubectl apply -f $traefik_yaml done Updates Updated URL for metallb install yaml file to use latest instead of pinning a specific version. k3s is actively being updated and the old version no longer worked. ","permalink":"https://unixorn.github.io/post/k3s-on-arm/","summary":"\u003ch2 id=\"why-k3s-and-not-stick-with-k8s\"\u003eWhy k3s and not stick with k8s?\u003c/h2\u003e\n\u003cp\u003eI wanted to experiment with \u003ca href=\"https://k3s.io\"\u003ek3s\u003c/a\u003e. They package everything you need in a single binary, don\u0026rsquo;t package in deprecated parts of k8s, and it works on Intel, ARMv7 and ARM64. It seemed like it\u0026rsquo;d be a less painful way to runn Kubernetes on my ARM cluster.\u003c/p\u003e","title":"Trying K3s on ARM, Part 1"},{"content":"Get your money from the Equifax settlement regarding the 2017 security breach.\nEligibility check site Settlement Site Your settlement options are to take a few months of their shitty credit monitoring that costs them nothing to provide, or to collect $125 you can spend anywhere on things that are actually of value to you.\nIn addition to the $125, if you spent up to 10 hours doing things like resetting all your passwords, setting up credit freezes, checking your bank accounts, credit cards, stock trading sites \u0026amp; etc for fraudulent transactions, you can get some extra money to compensate you for all that wasted time without having to submit paperwork to them about it. If you wasted more than 10 hours dealing with the fallout from their incompetence, you\u0026rsquo;ll have to submit paperwork for the extra hours.\nWhat I really want is an option of \u0026ldquo;never collect any data about me again you incompetent assholes, and delete all the data you have on me now\u0026rdquo;, but I had to settle for cash.\n","permalink":"https://unixorn.github.io/post/equifax-2017-breach-settlement/","summary":"\u003cp\u003eGet your money from the Equifax settlement regarding the 2017 security breach.\u003c/p\u003e","title":"Equifax 2017 Breach Settlement Information"},{"content":"One of the reasons I set up my cluster was that I\u0026rsquo;m running out of space on my NAS. I don\u0026rsquo;t want to buy a whole new chassis, and while I could have put individual file shares on each cluster node, that would be both inconvenient and not provide any data redundancy without a lot of brittle home-rolled hacks to rsync data from node to node. And since distributed file systems are a thing, I\u0026rsquo;d rather not resort to hacks.\nAlternatives Considered To make a long story short, I considered ceph, glusterfs, lizardfs \u0026amp; moosefs.\nceph Ceph\u0026rsquo;s published specs show that it basically won\u0026rsquo;t fit in my ODROID nodes. They say metadata servers need 1GB, OSDs need another 500MB, and the HC2s in my cluster only have 2GB of onboard RAM, and I\u0026rsquo;m running k8s on them too, so Ceph is out.\nGlusterFS GlusterFS looked nice at first, but it\u0026rsquo;s both less flexible (you can\u0026rsquo;t add single drives to the storage cluster) and less performant (by a factor of 2 compared to MooseFS) on my HC2 hardware.\nLizardFS LizardFS is a fork of MooseFS, and when I was trying to install it in my cluster I had a hard time finding debs for it. When I was looking for them online, I found a lot of complaints about performance issues on ARM, so that eliminated it from consideration.\nMooseFS There were a lot of things to like about MooseFS for my use case on my hardware.\nThere were prebuilt ARM debs on their site, in a handy PPA. It was twice as fast on my hardware as GlusterFS. I can add individual drive bricks to the cluster, even though my storage policies (more on them below) all require multiple replicas of data in the filesystem they\u0026rsquo;re applied to. The memory requirements are small enough that I can run it on the same nodes that I\u0026rsquo;m running kubernetes on. It dynamically balances disk usage across the bricks - when I added a third brickserver to my cluster, moosefs shuffled replica chunks over to it until all three servers had a roughly equal usage percentage. It allows custom storage policies: You can label storage bricks (say SSD as label A, spinning disks as label B) and use the labels in policy definitions. It\u0026rsquo;s flexible - you can create policies with different replication requirements and assign those on a per-directory or even per-file basis. By referring to brick labels, you can do things like create a policy that requires that at file creation, one replica be written to SSD and one to spinning disk, and then after that initial write is complete (so it can report back to the writing process that the write is done), that it then try to make sure there is a third copy so that there are two copies on spinning disks and one on SSD. You can make policies that change replication policies after user-specified amounts of time - so maybe your policy is that new files get one copy on SSD and 2 on spinning disk, but after 30 days, switch to one copy on a regular spinning disk and two on bigger slower drives. Installing You\u0026rsquo;re going to need a moosefs master, chunkservers for each machine that hosts drives, and should run a metadata backup server. You may also want to run the cgi to visualize the cluster status.\nPre-Requisites Static IPs for the cluster nodes, preferably with DNS entries. Setting this up is out of scope for this post. Decide which nodes will just be chunkservers, which will be the master, and optionally which are going to be metadata servers and cgi servers. You can run the master, metadata and cgi servers on machines that are also chunkservers. All servers Add the MooseFS PPA Add a file, /etc/apt/sources.list.d/moosefs.list, with the following contents deb http://ppa.moosefs.com/moosefs-3/apt/raspbian/jessie jessie main Run wget -O - https://ppa.moosefs.com/moosefs.key | sudo apt-key add - to add the moosefs PPA key\nRun apt-get update\nMaster Server Install the master server software on one of your nodes with apt install moosefs-master. Do this first, the chunkservers will need to communicate with it.\nOptionally install the cgi server with apt install moosefs-cgiserv\nConfigure /etc/mfs/mfsmaster.cfg and /etc/mfs/mfsexports.cfg. Start by copying /etc/mfs/mfsmaster.cfg.sample and /etc/mfs/mfsexports.cfg.sample.\nSet the master software to start on boot with systemctl enable moosefs-master. If you installed the cgi server, enable it too with systemctl enable moosefs-cgiserv.\nStart the master and cgi server with systemctl start moosefs-master \u0026amp;\u0026amp; systemctl start moosefs-cgiserv.\nChunkservers For each of your chunkservers, take the following steps:\nInstall moosefs software Install the software with apt install moosefs-chunkserver.\nConfigure which drives to use for storage Make a directory to store the moosefs data. On my HC2 instances, I mount the data drives on /mnt/sata, and keep the raw mfs data in /mnt/sata/moosefs\nConfigure /etc/mfs/mfshdd.cfg. There\u0026rsquo;s an example in /etc/mfs/mfshdd.cfg.sample - add one line per directory to be shared.\nIn my case, I want to keep 50 gigs free on the drive, so my entry in mfshdd.cfg is\n/mnt/sata/moosefs -50GB Configure the chunkserver options Copy /etc/mfs/mfschunkserver.cfg.sample to /etc/mfs/mfschunkserver.cfg and edit it to meet your needs - at a minimum, you\u0026rsquo;ll need to set MASTER_HOST = yourmaster.example.com\nEnable and start the chunkserver Set up the chunkserver to start at boot, and start it now -\nsystemctl enable moosefs-chunkserver \u0026amp;\u0026amp; systemctl start moosefs-chunkserver Mounting the filesystem Now that the chunkservers are talking to the master, you can set up automounting it on your nodes.\nFirst, install the client software - sudo apt install -y moosefs-client\nSecond, make a mountpoint. On my nodes, I\u0026rsquo;m using /data/squirrel, so sudo mkdir -p /data/squirrel\nFinally, create a systemd unit file so that the filesystem mounts every boot. I want to be able to use hostPath directives in my kubernetes deployments, so I want it to start before docker and kubelet. Make a file, /etc/systemd/system/yourcluster-mfsmount.service with the following content (replace /mountpoint with whatever mountpoint you\u0026rsquo;re using):\n# Original source: https://sourceforge.net/p/moosefs/mailman/message/29522468/ [Unit] Description=MooseFS mounts After=syslog.target network.target ypbind.service moosefs-chunkserver.service moosefs-master.service Before=docker.service kubelet.service [Service] Type=forking TimeoutSec=600 ExecStart=/usr/bin/mfsmount /mountpoint -H YOUR_MASTER_SERVER ExecStop=/usr/bin/umount /mountpoint [Install] WantedBy=multi-user.target Enable it so it starts every boot:\nsystemctl enable yourcluster-mfsmount \u0026amp;\u0026amp; systemctl start yourcluster-mfsmount ","permalink":"https://unixorn.github.io/post/adding-a-distributed-filesystem-to-cluster/","summary":"\u003cp\u003eOne of the reasons I set up my cluster was that I\u0026rsquo;m running out of space on my NAS. I don\u0026rsquo;t want to buy a whole new chassis, and while I could have put individual file shares on each cluster node, that would be both inconvenient and not provide any data redundancy without a lot of brittle home-rolled hacks to \u003ccode\u003ersync\u003c/code\u003e data from node to node. And since distributed file systems are a thing, I\u0026rsquo;d rather not resort to hacks.\u003c/p\u003e","title":"Adding a Distributed Filesystem to the Cluster"},{"content":"I had a bunch of cruft in start scripts that would determine the IP of my laptop and set it as an environment variable to pass to docker-compose and it turns out that with Docker for Mac you don\u0026rsquo;t have to do all that work - just use docker.for.mac.localhost as a hostname.\n","permalink":"https://unixorn.github.io/post/til-about-docker-mac/","summary":"I had a bunch of cruft in start scripts that would determine the IP of my laptop and set it as an environment variable to pass to docker-compose and it turns out that with Docker for Mac you don\u0026rsquo;t have to do all that work - just use docker.for.mac.localhost as a hostname.","title":"Things I Learned About Docker Mac"},{"content":"I\u0026rsquo;m not sure if this is just macOS Mojave being flaky or some interaction with the security malware installed on my work laptop, but it refuses to connect to some captive portals.\nThe problem is that it refuses to render the captive portal login page, so you can\u0026rsquo;t agree to their TOS, and don\u0026rsquo;t get a DHCP address.\nThere\u0026rsquo;s a workaround, you can open Apple\u0026rsquo;s hotspot detection page with Safari directly by running\nopen -a \u0026quot;Safari\u0026quot; \u0026quot;http://captive.apple.com/hotspot-detect.html\u0026quot;\nin a terminal window.\nThat\u0026rsquo;s annoying to remember (and honestly, most of the time I don\u0026rsquo;t have Safari running) so I\u0026rsquo;ve added it to my tumult zsh plugin for macOS here.\n","permalink":"https://unixorn.github.io/post/fix-macos-captive-portal-issue/","summary":"\u003cp\u003eI\u0026rsquo;m not sure if this is just macOS Mojave being flaky or some interaction with the security malware installed on my work laptop, but it refuses to connect to some captive portals.\u003c/p\u003e","title":"Fix macOS Captive WIFI Portal Issue"},{"content":"I realized I forgot to include a parts list for the cluster in my ARM cluster post (all prices are as of March 3rd, 2019), so here we go.\nThree Odroid HC2 ARM SBCs @ $54.95 each. These have gigabit ethernet ports, a SATA-3 port for 3.5 inch or 2.5 inch HDD/SSD drives, Samsung Exynos5422 Cortex-A15 2Ghz and Cortex-A7 Octa core 32bit ARM CPUs, with four 2GHz cores and four 1.4GHz cores and a UHS-1 compatible microSD slot that supports up to 128GB/SDXC. Three 12V/2A Power supplies @ $5.95 each and three power cords for the Odroids (They\u0026rsquo;re sold separately) @ $1.95 Three microSD cards - I used some I had already - they\u0026rsquo;re cheap, so get at least 32GB though 16GB works too. One 8 port Ubiquiti UniFi Switch 8 60W (US-8-60W) @ $109.59, though really any gigabit switch will do. I didn\u0026rsquo;t bother with cases for mine, I wanted to maximize airflow since they don\u0026rsquo;t have onboard fans.\n","permalink":"https://unixorn.github.io/post/arm_cluster_parts_list/","summary":"\u003cp\u003eI realized I forgot to include a parts list for the cluster in my \u003ca href=\"https://unixorn.github.io/blog/in_the_beginning_there_was_bare_metal/\"\u003eARM cluster post\u003c/a\u003e (all prices are as of March 3rd, 2019), so here we go.\u003c/p\u003e","title":"Parts list for the ARM cluster"},{"content":"I recently decided to set up a Kubernetes cluster in my basement, partly because I\u0026rsquo;d never set a cluster up from scratch by myself, and partly because my existing NAS was beginning to run out of headroom.\nFor a variety of reasons, I decided to use ODROID HC2 boards. They\u0026rsquo;ve got gigabit ethernet, eight CPU cores, 2 GB RAM and a SATA-3 port for directly connecting a hard drive, which I wanted so I could use them as file server bricks. In a future post I will detail how I set up a distributed filesystem across the cluster.\nEDIT - Added a link to the parts list, and added instructions for finding the new machine on your network.\nSetting up an ODROID HC2 cluster These notes should also work on an Odroid HC1 or XU4.\nInstall Debian Stretch I used meveric\u0026rsquo;s debian-stretch ISO from https://oph.mdrjr.net/meveric/images/Stretch/.\nI used Etcher to burn the debian-stretch ISO to an microSD card.\nFlash your microSD card, plug it into the HC2 and attach a SATA drive to your HC2 if you\u0026rsquo;re going to use one, then connect it to your switch and power up. It will get an IP address with DHCP. Since they don\u0026rsquo;t have a video connector, you\u0026rsquo;ll have to scan your network to figure out what IP it got.\nFind the Odroid on your network nmap You can use nmap to find the Odroid machines. Assuming your network is 10.0.0.1-254, you can scan the network with nmap -sP -n 10.0.0.0/24 | grep -v Host. Look for systems that show Wibrain in their MAC Address line.\nAngry IP Scanner If you\u0026rsquo;re not comfortable with nmap, I recommend Angry IP Scanner to find the new machine on your network because you can configure (select Fetchers in the Angry IP Scanner Menu on macOS, then add MAC Vendor to the selected fetchers) it to show the MAC vendor of your ethernet card - the ODroids show will show up as WIBRAIN.\nLogin as root, password odroid.\nChange your root password! Don\u0026rsquo;t skip this just because you\u0026rsquo;re running this on an internal-only network. The default root password for the image is well known, so run passwd root to change it so you\u0026rsquo;re not vulnerable if you accidentally open up your WIFI.\nInstall your updates Install ISOs are inevitably out of date, but that\u0026rsquo;s ok, we\u0026rsquo;ll begin by updating all the installed packages.\napt-get update \u0026amp;\u0026amp; apt-get upgrade \u0026amp;\u0026amp; apt-get dist-upgrade\nInstall useful tooling Let\u0026rsquo;s also add some useful tools to the machine.\napt-get install -y dnsutils git htop lshw man net-tools rsync sudo\nNow we\u0026rsquo;re ready to install Docker and Kubernetes.\nInstall docker-ce Install the docker-ce pre-requisites apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg2 \\ software-properties-common Partition \u0026amp; Format the drive Install pre-requisites Check what\u0026rsquo;s on your drive with lshw -C disk\nFor the sake of these examples, we\u0026rsquo;ll assume the SATA drive is /dev/sda\nFormat the drive If you didn\u0026rsquo;t add a SATA drive to your Odroid, you can skip this section.\nFirst partition it fdisk /dev/sda list all the existing partitions with the p command Remove any existing partitions with the d command Create a new partition with the n command Write the new partition table to disk with the w command Now format it mkfs.ext4 /dev/sda1 Configure the system to automatically mount the drive Get the UUID with blkid | grep /dev/sda. You\u0026rsquo;ll see something like /dev/sda1: UUID=\u0026quot;abcdabcd-abcd-11bb-9343-9089b93bbb72\u0026quot; TYPE=\u0026quot;ext4\u0026quot; PARTUUID=\u0026quot;13371337-abcd-1234-aa00-abcd1234abcd1234\u0026quot; Create a mount point to mount your filesystem. I picked /mnt/sata and created it with mkdir -p /mnt/sata Add an entry to /etc/fstab. Use your editor of choice to add a line UUID=\u0026quot;abcdabcd-abcd-11bb-9343-9089b93bbb72\u0026quot; /mnt/sata ext4 defaults 0 2. Use the UUID from step 1, not the example one here. You should now be able to mount the drive with mount /mnt/sata. If it succesfully mounts, it should show up after a reboot.\nForce a static IP for the node(s) Your best option is to assign your nodes static IPs on your DHCP server. This way you have one place to assign IPs, and if you need to reassign them you can change them all on the DHCP server and within a few minutes (if you\u0026rsquo;re in a rush, reboot your nodes) your nodes will switch to the new addreses.\nIf you can\u0026rsquo;t assign static addresses in your DHCP server on your router, it\u0026rsquo;s garbage and consider replaceing it. Until you do, you\u0026rsquo;ll have to hard code them on the nodes.\nFirst, back up the current network config with cp /etc/network/interfaces /etc/network/interfaces-original\nNow edit /etc/network/interfaces and put in:\n# Ethernet adapter 0 auto eth0 allow-hotplug eth0 #no-auto-down eth0 iface eth0 inet static address 192.168.1.100 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 8.8.8.8 8.8.4.4 # Or use your own by uncommenting below # dns-nameservers 192.168.1.1 Disable swap Kubernetes doesn\u0026rsquo;t like swap, so disable it with swapoff -a.\nInstall Docker and Kubernetes Docker Install the docker apt signing GPG key curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -\necho \u0026quot;deb [arch=armhf] https://download.docker.com/linux/debian $(lsb_release -cs) stable\u0026quot; \u0026gt; /etc/apt/sources.list.d/docker.list\nInstall pip \u0026amp; docker-compose apt-get update \u0026amp;\u0026amp; apt-get install -y python3-pip \u0026amp;\u0026amp; pip3 install setuptools docker-compose\nInstall docker apt-get install -y docker-ce --no-install-recommends\nConfirm Docker is working docker run hello-world\nYou should see something similar to this:\nroot@rodan:~# docker run hello-world Unable to find image \u0026#39;hello-world:latest\u0026#39; locally latest: Pulling from library/hello-world c1eda109e4da: Pull complete Digest: sha256:2557e3c07ed1e38f26e389462d03ed943586f744621577a99efb77324b0fe535 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (arm32v7) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Install Kubernetes Install the k8s repository key curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\nAdd the k8s apt repo cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubernetes sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl Uptodate at https://kubernetes.io/docs/setup/independent/\nThis is pretty tedious, is there an easier way? All this is tedious and prone to mistyping, especially if you\u0026rsquo;ve got multiple nodes to make into a cluster, so I\u0026rsquo;ve put everything except the network setup and disk formatting/mounting into a handy helper script, borg-odroid.\nYou can copy borg-odroid to a new machine after the first boot and it will bring the machine\u0026rsquo;s debian install up to date, then install docker \u0026amp; kubernetes and some other handy support tools.\nConfigure your Kubernetes Cluster First - did you configure your kubernetes nodes to use static addresses? You will have issues if you didn\u0026rsquo;t.\nInitialize the cluster on your master node By default, Flannel requires we use 10.244.0.0/16 for our CIDR when initializing the cluster, because the flannel configuration we\u0026rsquo;re going to install later expects that CIDR - sure, we could change all the references to it, but that is just going to give us chances to break it.\nkubeadm init --pod-network-cidr=10.244.0.0/16\nNote: It is normal to see your master node as NotReady if you run kubectl get nodes before setting up networking.\nSetup your config rm -rf ~/.kube/ \u0026amp;\u0026amp; mkdir ~/.kube \u0026amp;\u0026amp; cp /etc/kubernetes/admin.conf $HOME/.kube/config\nSet up Networking Set /proc/sys/net/bridge/bridge-nf-call-iptables to 1 by running sysctl net.bridge.bridge-nf-call-iptables=1 to pass bridged IPv4 traffic to iptables’ chains. Flannel supports the ARM architecture, so kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Allow pods to run on master node If you only have one node in the cluster, k8s won\u0026rsquo;t run pods on the master node. Disable tainting with kubectl taint nodes --all node-role.kubernetes.io/master-\nAdd worker nodes On the master, run kubeadm token create --print-join-command to generate an add command, then run that on the worker. It will look something like:\nkubeadm join --token SUPERSECRET 10.9.8.7:6443 --discovery-token-ca-cert-hash sha256:1234567890ABCDEF1234567890ABCDEF1234567890ABCDEF1234567890ABCDEF Install helm helm init --tiller-image=jessestuart/tiller:v2.9.1 --upgrade\nSet up the dashboard DASHSRC=https://raw.githubusercontent.com/kubernetes/dashboard/master curl -sSL $DASHSRC/src/deploy/recommended/kubernetes-dashboard-arm-head.yaml | kubectl apply -f - Updates Updated the docker install to use --no-install-recommends - this keeps it from attempting to install the aufs-dkms package, which causes error messages (but docker still works) on my Raspberry Pi 4 because the necessary kernel module isn\u0026rsquo;t provided. Added instructions for adding new worker nodes and disabling swap ","permalink":"https://unixorn.github.io/post/in_the_beginning_there_was_bare_metal/","summary":"\u003cp\u003eI recently decided to set up a Kubernetes cluster in my basement, partly because I\u0026rsquo;d never set a cluster up from scratch by myself, and partly because my existing NAS was beginning to run out of headroom.\u003c/p\u003e","title":"Getting an ARM kubernetes cluster up and running"},{"content":"I\u0026rsquo;m an SRE in Denver for Twilio.\nI like cooking, photography, and working on distributed infrastructure.\nI\u0026rsquo;m the organizer for Denver CoffeeOps, which currently meets online - the meetup link has the Google Meet details.\nI have a variety of DevOps (and DevOOPs) themed swag on redbubble, if you\u0026rsquo;d like to buy something and contribute to my hardware habit, or you can contribute at https://www.patreon.com/unixorn.\n","permalink":"https://unixorn.github.io/about-orig/","summary":"I\u0026rsquo;m an SRE in Denver for Twilio.\nI like cooking, photography, and working on distributed infrastructure.\nI\u0026rsquo;m the organizer for Denver CoffeeOps, which currently meets online - the meetup link has the Google Meet details.\nI have a variety of DevOps (and DevOOPs) themed swag on redbubble, if you\u0026rsquo;d like to buy something and contribute to my hardware habit, or you can contribute at https://www.patreon.com/unixorn.","title":""},{"content":"About Me I\u0026rsquo;m an SRE in Denver for ZScaler.\nI like cooking, photography, home automation and working on distributed infrastructure.\nYou can find me online at:\nMastodon: @unixorn@hachyderm.io I\u0026rsquo;m the organizer for the Denver CoffeeOps meetup, which currently meets online - the meetup link details are posted in the Colorado channels of both the hangops and coffeeops slacks.\nI have a variety of DevOPS (and DevOOPs) themed swag on redbubble and spreadshirt, if you\u0026rsquo;d like to buy something and contribute to me buying new hardware to write about.\nYou can also contribute at https://www.patreon.com/unixorn, or by getting me something from my Amazon Wishlist, or the wish list of IOT devices I want to test and post about.\n","permalink":"https://unixorn.github.io/about/","summary":"About Me I\u0026rsquo;m an SRE in Denver for ZScaler.\nI like cooking, photography, home automation and working on distributed infrastructure.\nYou can find me online at:\nMastodon: @unixorn@hachyderm.io I\u0026rsquo;m the organizer for the Denver CoffeeOps meetup, which currently meets online - the meetup link details are posted in the Colorado channels of both the hangops and coffeeops slacks.\nI have a variety of DevOPS (and DevOOPs) themed swag on redbubble and spreadshirt, if you\u0026rsquo;d like to buy something and contribute to me buying new hardware to write about.","title":""},{"content":"Email me at blog @ unixorn.net.\nYou can find me around the internet at:\nunixorn on GitHub @unixorn@mstdn.social on Mastodon I\u0026rsquo;m unixorn on redbubble apeseekingknowledge on Instagram. curiousbiped ~on Twitter~ deprecated, use Mastodon instead ","permalink":"https://unixorn.github.io/contact/","summary":"Email me at blog @ unixorn.net.\nYou can find me around the internet at:\nunixorn on GitHub @unixorn@mstdn.social on Mastodon I\u0026rsquo;m unixorn on redbubble apeseekingknowledge on Instagram. curiousbiped ~on Twitter~ deprecated, use Mastodon instead ","title":""},{"content":"Ziggy Stardust\n","permalink":"https://unixorn.github.io/jfc/","summary":"Ziggy Stardust","title":""},{"content":"Some projects I maintain and/or created apgar - Apgar is a quick and dirty health-check driver written in Go to prevent dependency conflicts. It is explicitly designed to be simple and not interfere with anything else running on your servers. awesome-zsh-plugins - A list of ZSH frameworks, plugins, themes, tab-completions and tutorials. dotfiles.github.io - The unofficial guide to dotfiles on GitHub. online-devops-meetups - A list of free online devops meetups. git-extra-commands - A collection of useful extra git scripts I\u0026rsquo;ve discovered or written, packaged for ease of use with shell frameworks. ha-mqtt-discoverable - A python module that lets you create sensor and device MQTT topics that will automatically be detected by Home Asssistant. sysadmin-reading-list - A reading/viewing list for larval stage SRE/DevOps people. tumult - Tumult is a collection of macOS-specific functions and scripts for your command-line environment. It is packaged as a ZSH plugin, but can be used in bash, fish or other shells as well. Works With Home Assistant - Not all IOT devices that claim to work with HA actually work well. This is a list of stuff that has been vouched to work well with Home Assistant. zsh-quickstart-kit - A simple quick start for switching to ZSH. Includes a curated list of plugins that\u0026rsquo;s easily overridden if you want to change it, is easily customizable without needing to maintain a fork, and tweaks the setup to allow history de-duplication, history sharing across shells on the same machine, and on macOS will load a bunch of command line tools for manipulating your Mac. ","permalink":"https://unixorn.github.io/projects/","summary":"Some projects I maintain and/or created apgar - Apgar is a quick and dirty health-check driver written in Go to prevent dependency conflicts. It is explicitly designed to be simple and not interfere with anything else running on your servers. awesome-zsh-plugins - A list of ZSH frameworks, plugins, themes, tab-completions and tutorials. dotfiles.github.io - The unofficial guide to dotfiles on GitHub. online-devops-meetups - A list of free online devops meetups. git-extra-commands - A collection of useful extra git scripts I\u0026rsquo;ve discovered or written, packaged for ease of use with shell frameworks.","title":""}]